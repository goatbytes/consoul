{
  "created_at": "2025-11-16T21:24:08.826758",
  "updated_at": "2025-11-20T04:23:22.880922Z",
  "id": "SOUL-128",
  "uuid": "4606721a-d2e6-4cba-8912-cf4fd4a1509d",
  "title": "Add MLX model conversion feature for local HuggingFace models",
  "description": "**As a** Consoul user on macOS with local HuggingFace models\n**I want** to convert my existing PyTorch models to MLX format\n**So that** I can run them locally without PyTorch segfaults using Apple Silicon optimization\n\n**Context:**\nResearch has shown that PyTorch 2.5+ has severe segfault issues on macOS Apple Silicon (including M4 chips) with no reliable fix from the PyTorch team. The environment variable workarounds (OMP_NUM_THREADS=1) don't work consistently with newer versions.\n\nMLX is Apple's official ML framework optimized for Apple Silicon that provides:\n- Native M-series chip support (no OpenMP conflicts)\n- 4-bit quantization for reduced RAM usage\n- Unified memory for seamless CPU-GPU collaboration\n- 10x faster performance than PyTorch on M-series chips\n- Zero segfault issues\n\nCurrent state:\n- MLX provider support already implemented (COMPLETED)\n- MLX shows pre-converted mlx-community models\n- Users have local PyTorch models (e.g., Qwen/Qwen3-8B) that cause segfaults\n- mlx-lm provides conversion tool: mlx_lm.convert\n\nUser's local HuggingFace cache contains:\n- Qwen/Qwen3-8B (16.4GB) - causes segfaults with PyTorch\n- moonshotai/Kimi-Linear-48B-A3B-Instruct (98.3GB)\n- Other models that could benefit from MLX conversion\n\n**Technical Notes:**\n- MLX conversion command: python -m mlx_lm convert --hf-path <model> --mlx-path <output> -q\n- Conversion options: --q-bits (4/8), --q-group-size, --dtype (float16/bfloat16/float32)\n- Default quantization is 4-bit (reduces size ~4x)\n- Converted models stored in ~/.cache/mlx/ or custom location\n- src/consoul/ai/providers.py:959-1006 - Current MLX provider implementation\n- src/consoul/tui/widgets/model_picker_modal.py:615-647 - MLX model list\n- Need to scan both HuggingFace cache and MLX cache\n- Integration point: Add \"Convert to MLX\" option in model picker\n\n**Acceptance Criteria:**\n\n**Given** I have Qwen/Qwen3-8B in my HuggingFace cache\n**When** I open the MLX tab in model picker\n**Then** I see both mlx-community models and my local PyTorch models with \"Convert\" option\n\n**Given** I select a local PyTorch model and click \"Convert to MLX\"\n**When** conversion starts\n**Then** I see progress indicator (conversion can take 5-15 minutes)\n\n**Given** conversion completes successfully\n**When** I refresh the model list\n**Then** the converted model appears in MLX models list without \"Convert\" option\n\n**Given** I select a converted MLX model\n**When** I send a message\n**Then** the model runs locally without PyTorch segfaults\n\n**Given** conversion fails (out of disk space, invalid model)\n**When** error occurs\n**Then** I see clear error message with troubleshooting steps\n\n**Testing Considerations:**\n- Test conversion with small model (gpt2) for speed\n- Test 4-bit vs 8-bit quantization\n- Test disk space validation before conversion\n- Test conversion cancellation (Ctrl+C)\n- Test with already-converted models (skip conversion)\n- Mock conversion for unit tests (subprocess call)\n- Integration test with actual small model conversion\n- Test MLX cache detection and model listing\n- Verify converted models work with ChatMLX\n\n**Implementation Hints:**\n1. Add function get_mlx_models_from_cache() similar to get_gguf_models_from_cache()\n2. Scan both ~/.cache/mlx/ and custom MLX_CACHE_DIR\n3. Add function get_convertible_hf_models() that lists PyTorch models\n4. In model picker, show \"\ud83d\udd04 Convert to MLX\" button for PyTorch models\n5. Implement convert_to_mlx(hf_path, mlx_path, quantize=True, q_bits=4)\n6. Use subprocess to run: mlx_lm.convert --hf-path {hf_path} --mlx-path {mlx_path} -q --q-bits {q_bits}\n7. Show progress with Rich.Progress or Textual ProgressBar\n8. Store conversion metadata (original model, date, size) in .mlx_metadata.json\n9. Update MLX model list to merge mlx-community + local converted models\n10. Add config option: mlx.cache_dir (default: ~/.cache/mlx)\n\nFiles to create/modify:\n- src/consoul/ai/providers.py - Add get_mlx_models_from_cache(), get_convertible_hf_models(), convert_to_mlx()\n- src/consoul/tui/widgets/model_picker_modal.py - Add convert button and progress UI\n- src/consoul/config/models.py - Add MLXModelConfig.cache_dir field\n\nReference implementations:\n- get_gguf_models_from_cache() in providers.py:211-292\n- LlamaCpp auto-detection pattern in providers.py:884-957\n- Model conversion similar to: mlx-lm examples\n\n**Dependencies:**\n- MLX provider support (COMPLETED)\n- mlx-lm package installed (COMPLETED)\n- HuggingFace models in local cache\n\n**Benefits:**\n- Users can convert their 16GB Qwen3-8B to ~4GB MLX model\n- Zero segfaults with converted models\n- 10x faster inference on Apple Silicon\n- No need to download mlx-community versions if user has PyTorch model\n- Works offline after conversion",
  "status": "done",
  "type": "feature",
  "priority": "high",
  "labels": [
    "mlx",
    "ai",
    "providers",
    "macos",
    "enhancement"
  ],
  "assignee": "jared@goatbytes.io",
  "reporter": "jared@goatbytes.io",
  "blocked_by": [],
  "blocks": [],
  "comments": [],
  "attachments": [],
  "attachment_count": 0,
  "comment_count": 0,
  "story_points": 8,
  "order": 0,
  "custom_fields": {},
  "_archive_metadata": {
    "archived_at": "2025-11-19T20:23:41.532801",
    "archived_from_status": "done",
    "archived_by": "gira-cli"
  }
}