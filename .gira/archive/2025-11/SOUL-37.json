{
  "created_at": "2025-11-09T20:12:23.492250",
  "updated_at": "2025-11-11T04:27:46.244342Z",
  "id": "SOUL-37",
  "uuid": "8071234c-edc6-49a6-b67d-af65da70e628",
  "title": "Create StreamingResponse widget for real-time AI output",
  "description": "# Create StreamingResponse Widget for Real-Time AI Output\n\n## User Story\n**As a** Consoul user\n**I want** to see AI responses stream in real-time token-by-token\n**So that** I get immediate feedback and can follow the AI's thinking process\n\n## Description\n\nCreate the StreamingResponse widget that handles real-time display of streaming AI tokens. This is a critical performance component that must buffer tokens, debounce markdown re-rendering, and provide smooth visual feedback without UI freezes.\n\n## Context\n\nFrom TUI_RESEARCH.md \u00a76 \"Streaming Renderer Strategy\":\n- Buffer 200 characters before rendering\n- Debounce markdown updates to 120-150ms\n- Support 3 renderer modes: markdown, richlog, hybrid\n- Fallback to RichLog if markdown rendering is too slow\n- Show streaming cursor animation\n\n**Key Reference:** TUI_RESEARCH.md \u00a76.1 \"Prototype Strategy\"\n\n## Technical Notes\n\n**File:** `src/consoul/tui/widgets/streaming_response.py`\n\n**Implementation:**\n\n```python\nfrom textual.widgets import RichLog, Static\nfrom textual.reactive import reactive\nfrom rich.markdown import Markdown\nfrom rich.text import Text\nimport time\n\n\nclass StreamingResponse(Static):\n    \"\"\"Widget for displaying streaming AI responses.\n    \n    Buffers tokens and debounces markdown rendering for performance.\n    Falls back to plain text if markdown rendering is too slow.\n    \"\"\"\n    \n    BUFFER_SIZE = 200  # chars\n    DEBOUNCE_MS = 150  # milliseconds\n    \n    # Reactive state\n    streaming: reactive[bool] = reactive(False)\n    token_count: reactive[int] = reactive(0)\n    \n    def __init__(self, renderer: str = \"markdown\", **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.renderer_mode = renderer\n        self.token_buffer: list[str] = []\n        self.full_content = \"\"\n        self.last_render_time = 0.0\n        self.render_pending = False\n    \n    def on_mount(self) -> None:\n        \"\"\"Initialize streaming response widget.\"\"\"\n        self.border_title = \"Assistant\"\n        self.add_class(\"streaming-response\")\n    \n    async def add_token(self, token: str) -> None:\n        \"\"\"Add a streaming token to the response.\n        \n        Args:\n            token: Text token from AI stream\n        \"\"\"\n        self.token_buffer.append(token)\n        self.full_content += token\n        self.token_count += 1\n        self.streaming = True\n        \n        # Check if we should render\n        buffer_size = sum(len(t) for t in self.token_buffer)\n        current_time = time.time() * 1000  # ms\n        time_since_render = current_time - self.last_render_time\n        \n        if buffer_size >= self.BUFFER_SIZE or time_since_render >= self.DEBOUNCE_MS:\n            await self._render_content()\n    \n    async def _render_content(self) -> None:\n        \"\"\"Render buffered content based on renderer mode.\"\"\"\n        if not self.token_buffer:\n            return\n        \n        self.last_render_time = time.time() * 1000\n        \n        if self.renderer_mode == \"markdown\":\n            try:\n                md = Markdown(self.full_content)\n                self.update(md)\n            except Exception:\n                # Fallback to plain text if markdown fails\n                self.update(self.full_content)\n        else:\n            # Plain text mode\n            self.update(self.full_content)\n        \n        # Show cursor while streaming\n        if self.streaming:\n            cursor = Text(\"\u258c\", style=\"bold blink\")\n            # Append cursor (implementation depends on widget type)\n    \n    async def finalize_stream(self) -> None:\n        \"\"\"Finalize streaming and render final content.\"\"\"\n        self.streaming = False\n        await self._render_content()\n        self.border_title = f\"Assistant ({self.token_count} tokens)\"\n    \n    def reset(self) -> None:\n        \"\"\"Clear content and reset state.\"\"\"\n        self.token_buffer.clear()\n        self.full_content = \"\"\n        self.token_count = 0\n        self.streaming = False\n        self.update(\"\")\n```\n\n**CSS:**\n\n```css\nStreamingResponse {\n    width: 100%;\n    height: auto;\n    background: $secondary;\n    color: $text;\n    padding: 1 2;\n    border: tall $success;\n    border-title-color: $success;\n}\n\nStreamingResponse.streaming {\n    border: heavy $accent;\n    border-title-color: $accent;\n}\n\n.streaming-cursor {\n    background: $accent;\n    color: $surface;\n}\n```\n\n## Acceptance Criteria\n\n**Given** I create a StreamingResponse widget\n**When** I mount it\n**Then** it shows border title \"Assistant\" and is empty\n\n**Given** I add tokens one by one\n**When** buffer reaches 200 chars\n**Then** content is rendered with markdown\n\n**Given** I add tokens slowly\n**When** 150ms passes since last render\n**Then** content is rendered even if buffer < 200 chars\n\n**Given** streaming completes\n**When** I call finalize_stream()\n**Then** final content is rendered and streaming=False\n\n**Given** markdown rendering fails\n**When** rendering is attempted\n**Then** fallback to plain text rendering occurs\n\n## Testing Considerations\n\n**Test File:** `tests/tui/widgets/test_streaming_response.py`\n\n```python\nimport pytest\nfrom consoul.tui.widgets.streaming_response import StreamingResponse\n\n\nasync def test_streaming_response_mounts():\n    \"\"\"Test StreamingResponse initialization.\"\"\"\n    widget = StreamingResponse()\n    assert not widget.streaming\n    assert widget.token_count == 0\n\n\nasync def test_add_tokens():\n    \"\"\"Test adding tokens to stream.\"\"\"\n    widget = StreamingResponse()\n    \n    await widget.add_token(\"Hello\")\n    await widget.add_token(\" world\")\n    \n    assert widget.token_count == 2\n    assert widget.streaming is True\n    assert \"Hello world\" in widget.full_content\n\n\nasync def test_buffer_threshold_triggers_render():\n    \"\"\"Test rendering when buffer threshold reached.\"\"\"\n    widget = StreamingResponse()\n    \n    # Add enough tokens to exceed buffer\n    long_text = \"x\" * 250\n    await widget.add_token(long_text)\n    \n    # Should have rendered\n    assert widget.last_render_time > 0\n\n\nasync def test_finalize_stream():\n    \"\"\"Test finalizing stream.\"\"\"\n    widget = StreamingResponse()\n    \n    await widget.add_token(\"Test\")\n    await widget.finalize_stream()\n    \n    assert not widget.streaming\n    assert \"1 tokens\" in widget.border_title\n```\n\n## Implementation Hints\n\n1. Start with Static as base widget\n2. Implement token buffering logic\n3. Add debounced rendering with time check\n4. Implement markdown rendering with fallback\n5. Add streaming cursor animation\n6. Test with long streams (1000+ tokens)\n7. Measure render performance\n\n**Performance Testing:**\n\n```python\nasync def test_stream_performance():\n    \"\"\"Test streaming 1000 tokens doesn't freeze UI.\"\"\"\n    widget = StreamingResponse()\n    \n    start = time.time()\n    for i in range(1000):\n        await widget.add_token(\"token \")\n    elapsed = time.time() - start\n    \n    # Should complete in reasonable time\n    assert elapsed < 2.0  # 2 seconds max\n```\n\n## Files to Create/Modify\n\n**New Files:**\n- src/consoul/tui/widgets/streaming_response.py\n- tests/tui/widgets/test_streaming_response.py\n\n**Modified Files:**\n- src/consoul/tui/widgets/__init__.py\n- src/consoul/tui/css/main.tcss\n\n## Dependencies\n\n**Prerequisites:**\n- SOUL-32 complete (widgets/ directory)\n- TUI_RESEARCH.md streaming strategy\n\n**Blocks:**\n- SOUL-37 (MessageBubble integrates StreamingResponse)\n- SOUL-41 (AI provider integration uses streaming)",
  "status": "done",
  "type": "feature",
  "priority": "high",
  "labels": [
    "tui",
    "widgets",
    "streaming"
  ],
  "assignee": "jared@goatbytes.io",
  "reporter": "jared@goatbytes.io",
  "epic_id": "EPIC-003",
  "blocked_by": [],
  "blocks": [],
  "comments": [],
  "attachments": [],
  "attachment_count": 0,
  "comment_count": 0,
  "story_points": 8,
  "order": 0,
  "custom_fields": {},
  "_archive_metadata": {
    "archived_at": "2025-11-14T23:28:31.533637",
    "archived_from_status": "done",
    "archived_by": "gira-cli"
  }
}