{
  "created_at": "2025-11-09T20:15:05.999956",
  "updated_at": "2025-11-11T04:27:46.236773Z",
  "id": "SOUL-41",
  "uuid": "22ba0f4b-21a0-4b0f-8cf6-9146c719c247",
  "title": "Connect TUI to AI providers for streaming responses",
  "description": "# Connect TUI to AI Providers for Streaming Responses\n\n## User Story\n**As a** Consoul user\n**I want** the TUI to send my messages to AI providers and display streaming responses\n**So that** I can have real-time conversations with AI models\n\n## Description\n\nIntegrate the TUI widgets with the AI provider system (from EPIC-002) to enable streaming conversations. Wire up InputArea.MessageSubmit to AI providers and StreamingResponse to display tokens in real-time.\n\n## Context\n\nThis ticket connects:\n- InputArea widget \u2192 AI provider chat method\n- AI streaming tokens \u2192 StreamingResponse widget\n- Message history \u2192 ChatView with MessageBubbles\n- Profile/model selection \u2192 AI provider initialization\n\n**Prerequisites:**\n- SOUL-20 through SOUL-25 complete (AI providers)\n- SOUL-36 through SOUL-39 complete (TUI widgets)\n\n## Technical Notes\n\n**File:** `src/consoul/tui/app.py` (modifications)\n\n```python\nfrom consoul.ai import get_provider\nfrom consoul.tui.widgets import ChatView, InputArea, MessageBubble, StreamingResponse\n\n\nclass ConsoulApp(App[None]):\n    # ... existing code ...\n    \n    def __init__(self, **kwargs) -> None:\n        super().__init__()\n        # Initialize AI provider\n        self.provider = get_provider(\n            profile_name=self.current_profile,\n            model_name=self.current_model\n        )\n        self.current_streaming_widget: StreamingResponse | None = None\n    \n    def compose(self) -> ComposeResult:\n        yield Header()\n        yield ChatView(id=\"chat-view\")\n        yield InputArea(id=\"input-area\")\n        yield Footer()\n    \n    async def on_input_area_message_submit(\n        self, event: InputArea.MessageSubmit\n    ) -> None:\n        \"\"\"Handle user message submission.\"\"\"\n        # Get widgets\n        chat_view = self.query_one(\"#chat-view\", ChatView)\n        \n        # Add user message to chat\n        user_bubble = MessageBubble(\n            content=event.content,\n            role=\"user\",\n            show_metadata=self.config.tui.show_timestamps\n        )\n        await chat_view.add_message(user_bubble)\n        \n        # Create streaming response widget\n        streaming_widget = StreamingResponse(\n            renderer=self.config.tui.stream_renderer\n        )\n        await chat_view.add_message(streaming_widget)\n        self.current_streaming_widget = streaming_widget\n        \n        # Start streaming (disable input during streaming)\n        self.streaming = True\n        input_area = self.query_one(\"#input-area\", InputArea)\n        input_area.disabled = True\n        \n        # Stream AI response\n        try:\n            async for token in self.provider.stream_chat(event.content):\n                await streaming_widget.add_token(token)\n            \n            # Finalize stream\n            await streaming_widget.finalize_stream()\n            \n            # Convert to permanent MessageBubble\n            final_content = streaming_widget.full_content\n            assistant_bubble = MessageBubble(\n                content=final_content,\n                role=\"assistant\",\n                token_count=streaming_widget.token_count,\n                show_metadata=self.config.tui.show_timestamps\n            )\n            \n            # Replace streaming widget with permanent bubble\n            await streaming_widget.remove()\n            await chat_view.add_message(assistant_bubble)\n            \n        except Exception as e:\n            # Show error\n            error_bubble = MessageBubble(\n                content=f\"Error: {str(e)}\",\n                role=\"error\"\n            )\n            await streaming_widget.remove()\n            await chat_view.add_message(error_bubble)\n        \n        finally:\n            # Re-enable input\n            self.streaming = False\n            input_area.disabled = False\n            input_area.text_area.focus()\n            self.current_streaming_widget = None\n    \n    def action_cancel_stream(self) -> None:\n        \"\"\"Cancel active streaming (enhanced from Phase 1).\"\"\"\n        if self.streaming and self.current_streaming_widget:\n            # Stop the stream\n            self.streaming = False\n            \n            # Finalize partial response\n            self.current_streaming_widget.finalize_stream()\n            \n            # Re-enable input\n            input_area = self.query_one(\"#input-area\", InputArea)\n            input_area.disabled = False\n            input_area.text_area.focus()\n            \n            self.notify(\"Streaming cancelled\")\n```\n\n## Acceptance Criteria\n\n**Given** I type a message and press Enter\n**When** the message is submitted\n**Then** user message appears in ChatView and AI streaming begins\n\n**Given** AI is streaming a response\n**When** tokens arrive\n**Then** StreamingResponse updates in real-time (buffered/debounced)\n\n**Given** streaming completes\n**When** AI finishes response\n**Then** StreamingResponse is replaced with permanent MessageBubble\n\n**Given** streaming is active\n**When** I press Escape\n**Then** streaming is cancelled and input is re-enabled\n\n**Given** an error occurs during streaming\n**When** exception is caught\n**Then** error MessageBubble is displayed\n\n## Testing Considerations\n\n**Test File:** `tests/tui/test_ai_integration.py`\n\n```python\nfrom consoul.tui.app import ConsoulApp\nfrom consoul.tui.widgets.input_area import InputArea\n\n\nasync def test_message_submission():\n    \"\"\"Test user message submission flow.\"\"\"\n    app = ConsoulApp(test_mode=True)\n    \n    # Simulate user input\n    input_area = app.query_one(\"#input-area\", InputArea)\n    input_area.text_area.text = \"Hello AI\"\n    \n    # Submit\n    await input_area._send_message()\n    \n    # Check chat view has message\n    chat_view = app.query_one(\"#chat-view\")\n    assert chat_view.message_count == 1  # User message\n\n\nasync def test_streaming_response():\n    \"\"\"Test AI streaming response display.\"\"\"\n    app = ConsoulApp(test_mode=True)\n    \n    # Submit message\n    # ... trigger streaming ...\n    \n    # Verify streaming widget created\n    assert app.streaming is True\n    assert app.current_streaming_widget is not None\n\n\nasync def test_cancel_streaming():\n    \"\"\"Test cancelling active stream.\"\"\"\n    app = ConsoulApp(test_mode=True)\n    \n    # Start streaming\n    # ... trigger streaming ...\n    \n    # Cancel\n    app.action_cancel_stream()\n    \n    assert app.streaming is False\n```\n\n## Implementation Hints\n\n1. Wire InputArea.MessageSubmit to AI provider\n2. Create StreamingResponse widget for each AI response\n3. Implement token streaming loop\n4. Replace StreamingResponse with MessageBubble on completion\n5. Add error handling\n6. Test with all 4 AI providers\n7. Test cancel functionality\n\n**Provider Switching:**\n\n```python\ndef watch_current_profile(self, new_profile: str) -> None:\n    \"\"\"Reinitialize provider when profile changes.\"\"\"\n    self.provider = get_provider(\n        profile_name=new_profile,\n        model_name=self.current_model\n    )\n```\n\n## Files to Create/Modify\n\n**Modified Files:**\n- src/consoul/tui/app.py\n- tests/tui/test_ai_integration.py (new)\n\n## Dependencies\n\n**Prerequisites:**\n- SOUL-20 through SOUL-25 complete (AI providers)\n- SOUL-36 through SOUL-39 complete (TUI widgets)\n\n**Blocks:**\n- SOUL-42 (Database integration saves conversations)",
  "status": "done",
  "type": "feature",
  "priority": "high",
  "labels": [
    "tui",
    "ai-integration",
    "streaming"
  ],
  "assignee": "jared@goatbytes.io",
  "reporter": "jared@goatbytes.io",
  "epic_id": "EPIC-003",
  "blocked_by": [],
  "blocks": [],
  "comments": [],
  "attachments": [],
  "attachment_count": 0,
  "comment_count": 0,
  "story_points": 8,
  "order": 0,
  "custom_fields": {},
  "_archive_metadata": {
    "archived_at": "2025-11-14T23:28:30.993660",
    "archived_from_status": "done",
    "archived_by": "gira-cli"
  }
}