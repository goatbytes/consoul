{
  "created_at": "2025-11-29T13:26:30.610857",
  "updated_at": "2025-12-06T16:43:09.914379Z",
  "id": "SOUL-193",
  "uuid": "69d96aa6-1163-48c1-85cc-3a200777cfb5",
  "title": "Testing and validation of loading screen integration",
  "description": "**As a** Consoul developer\n**I want** comprehensive tests validating the loading screen integration\n**So that** we ensure reliability, performance, and error handling work correctly\n\n**Context:**\nAfter implementing all loading screen components (Phases 1-5), we need thorough testing to verify:\n- Fast initialization (<1s) still shows progress\n- Slow initialization (>5s) updates smoothly\n- All error scenarios are handled gracefully\n- No regressions in existing functionality\n- Performance improvements are measurable\n- Theme switching works post-init\n- Ctrl+C cancellation during loading\n\nThis phase includes unit tests, integration tests, performance benchmarks, and manual testing.\n\n**Technical Notes:**\n- Create tests/tui/test_loading_integration.py\n- Create tests/tui/test_async_initialization.py\n- Create tests/tui/test_error_screen.py\n- Add performance benchmarks for __init__ timing\n- Update existing tests that may be affected by async init\n- Create manual test checklist for QA\n\n**Acceptance Criteria:**\n\n**Given** all tests pass\n**When** I run pytest tests/tui/\n**Then** all loading screen tests complete successfully\n\n**Given** I run the app normally\n**When** initialization completes\n**Then** the app behaves exactly as before refactoring\n\n**Given** I simulate slow network\n**When** the app initializes\n**Then** progress updates are visible and smooth\n\n**Testing Considerations:**\n- Unit test each extracted initialization method independently\n- Test __init__ completes in <50ms (no heavy operations)\n- Integration test full initialization flow with mocked dependencies\n- Test all error scenarios (network, auth, config, database)\n- Verify no regression in existing functionality\n- Performance benchmark: measure startup time improvement\n- Visual test: verify loading animation renders correctly\n- Test cancellation (Ctrl+C) during each initialization step\n- Test retry mechanism on error screen\n- Test with various config options (auto-resume on/off, different providers)\n\n**Implementation Hints:**\n\n**Unit Tests:**\n```python\n# tests/tui/test_async_initialization.py\nimport pytest\nfrom unittest.mock import AsyncMock, MagicMock, patch\nfrom consoul.tui.app import ConsoulApp\n\n@pytest.mark.asyncio\nasync def test_load_config_method():\n    \"\"\"Test _load_config extracts config loading.\"\"\"\n    app = ConsoulApp(config=None, consoul_config=None, test_mode=True)\n\n    with patch('consoul.tui.app.load_config') as mock_load:\n        mock_load.return_value = MagicMock()\n        config = await app.run_in_thread(app._load_config)\n\n    assert config is not None\n    mock_load.assert_called_once()\n\n@pytest.mark.asyncio\nasync def test_initialize_ai_model_method():\n    \"\"\"Test _initialize_ai_model creates chat model.\"\"\"\n    app = ConsoulApp(config=None, consoul_config=None, test_mode=True)\n    mock_config = MagicMock()\n\n    with patch('consoul.tui.app.get_chat_model') as mock_get:\n        mock_get.return_value = MagicMock()\n        model = await app.run_in_thread(app._initialize_ai_model, mock_config)\n\n    assert model is not None\n    mock_get.assert_called_once()\n\ndef test_init_is_fast():\n    \"\"\"Test that __init__ completes quickly.\"\"\"\n    import time\n    start = time.time()\n\n    app = ConsoulApp(config=None, consoul_config=None, test_mode=True)\n\n    elapsed = time.time() - start\n    assert elapsed < 0.05, f\"__init__ took {elapsed}s, expected <50ms\"\n```\n\n**Integration Tests:**\n```python\n# tests/tui/test_loading_integration.py\nimport pytest\nfrom unittest.mock import AsyncMock, patch\nfrom consoul.tui.app import ConsoulApp\nfrom consoul.tui.loading import ConsoulLoadingScreen\n\n@pytest.mark.asyncio\nasync def test_full_initialization_flow():\n    \"\"\"Test complete initialization with loading screen.\"\"\"\n    app = ConsoulApp(config=None, consoul_config=None, test_mode=True)\n\n    # Mock all heavy operations\n    with patch.multiple(\n        app,\n        _load_config=AsyncMock(),\n        _initialize_ai_model=AsyncMock(),\n        _initialize_conversation=AsyncMock(),\n        _initialize_tool_registry=AsyncMock(),\n        _auto_resume_if_enabled=AsyncMock(),\n    ):\n        await app._async_initialize()\n\n    assert app._initialization_complete is True\n\n@pytest.mark.asyncio\nasync def test_progress_updates():\n    \"\"\"Test that progress updates are called correctly.\"\"\"\n    app = ConsoulApp(config=None, consoul_config=None, test_mode=True)\n    mock_screen = MagicMock(spec=ConsoulLoadingScreen)\n    app.screen = mock_screen\n\n    with patch.multiple(\n        app,\n        _load_config=AsyncMock(),\n        _initialize_ai_model=AsyncMock(),\n        _initialize_conversation=AsyncMock(),\n        _initialize_tool_registry=AsyncMock(),\n        _auto_resume_if_enabled=AsyncMock(),\n    ):\n        await app._async_initialize()\n\n    # Verify progress updates were called\n    assert mock_screen.update_progress.call_count >= 6\n\n    # Verify final progress is 100%\n    final_call = mock_screen.update_progress.call_args_list[-1]\n    assert final_call[0][1] == 100  # Second argument is percentage\n```\n\n**Error Handling Tests:**\n```python\n# tests/tui/test_error_screen.py\nimport pytest\nfrom consoul.tui.screens.error import InitializationErrorScreen\n\ndef test_error_screen_composition():\n    \"\"\"Test error screen renders correctly.\"\"\"\n    error = ValueError(\"Test error message\")\n    screen = InitializationErrorScreen(error=error)\n\n    # Compose should not raise\n    list(screen.compose())\n\ndef test_error_screen_shows_troubleshooting_tip():\n    \"\"\"Test that troubleshooting tips are shown.\"\"\"\n    error = FileNotFoundError(\"Config not found\")\n    screen = InitializationErrorScreen(error=error)\n\n    tip = screen._get_troubleshooting_tip(\"FileNotFoundError\")\n    assert \"config file\" in tip.lower()\n\n@pytest.mark.asyncio\nasync def test_initialization_shows_error_on_failure():\n    \"\"\"Test that error screen is shown on init failure.\"\"\"\n    app = ConsoulApp(config=None, consoul_config=None, test_mode=True)\n\n    with patch.object(app, '_load_config', side_effect=ValueError(\"Test error\")):\n        await app._async_initialize()\n\n    # Verify error screen was pushed\n    assert isinstance(app.screen, InitializationErrorScreen)\n```\n\n**Performance Tests:**\n```python\n# tests/tui/test_performance.py\nimport time\nimport pytest\nfrom consoul.tui.app import ConsoulApp\n\ndef test_init_performance():\n    \"\"\"Benchmark __init__ performance.\"\"\"\n    times = []\n    for _ in range(10):\n        start = time.time()\n        app = ConsoulApp(config=None, consoul_config=None, test_mode=True)\n        elapsed = time.time() - start\n        times.append(elapsed)\n\n    avg_time = sum(times) / len(times)\n    assert avg_time < 0.05, f\"Average __init__ time: {avg_time}s\"\n    print(f\"\\\\n\u2705 Average __init__ time: {avg_time*1000:.1f}ms\")\n```\n\n**Manual Test Checklist:**\n```markdown\n## Loading Screen Visual Tests\n- [ ] Fast init (<1s total) - progress should still be visible\n- [ ] Slow init (>5s) - progress updates smoothly\n- [ ] CODE_STREAM animation renders correctly\n- [ ] Progress bar animates smoothly\n- [ ] Messages update at each step\n- [ ] Fade-out animation is smooth (0.5s duration)\n\n## Error Scenarios\n- [ ] Invalid config file - shows error screen\n- [ ] Missing config file - shows error screen\n- [ ] Network failure - shows error screen\n- [ ] Invalid API key - shows error screen\n- [ ] Ctrl+C during load - cancels gracefully\n\n## Functionality Tests\n- [ ] Auto-resume with large conversation - doesn't freeze\n- [ ] Theme switching after load - works correctly\n- [ ] Multiple rapid starts/stops - no resource leaks\n- [ ] All tools load correctly\n- [ ] Chat works normally after init\n- [ ] System prompt is added correctly\n\n## Performance Tests\n- [ ] Startup feels instant (loading screen appears <50ms)\n- [ ] Progress is visible even with fast init\n- [ ] No UI freezing at any point\n- [ ] Memory usage is normal\n```\n\n**Test Execution:**\n```bash\n# Run all tests\npytest tests/tui/ -v\n\n# Run with coverage\npytest tests/tui/ --cov=consoul.tui --cov-report=html\n\n# Run performance benchmarks\npytest tests/tui/test_performance.py -v -s\n\n# Run manual tests\n# (Launch app and verify checklist items)\n```\n\n**Dependencies:**\n- Requires Phases 1-5 completion\n- Final phase before release",
  "status": "done",
  "type": "task",
  "priority": "high",
  "labels": [
    "tui",
    "testing",
    "qa"
  ],
  "assignee": "jared@goatbytes.io",
  "reporter": "jared@goatbytes.io",
  "blocked_by": [],
  "blocks": [],
  "comments": [],
  "attachments": [],
  "attachment_count": 0,
  "comment_count": 0,
  "story_points": 3,
  "order": 0,
  "custom_fields": {},
  "_archive_metadata": {
    "archived_at": "2025-12-16T03:35:59.284267+00:00",
    "archived_from_status": "done",
    "archived_by": "gira-cli"
  }
}