{
  "created_at": "2025-11-13T14:07:31.849630",
  "updated_at": "2025-11-29T23:34:06.501442Z",
  "id": "SOUL-92",
  "uuid": "c79db4ac-5d36-44dd-9dd7-dfee5936f716",
  "title": "Research image analysis tools for multimodal capabilities",
  "description": "Research adding vision/image analysis capabilities to Consoul.\n\nCurrent state:\n- Consoul supports text-only interactions\n- No multimodal capabilities (vision, image analysis)\n- Some LLM providers support vision (Claude 3.5, GPT-4V, Gemini)\n\nProposed features from cursor-agent:\n- query_images tool - Analyze images using LLM vision\n- Support multiple images in one query\n- Image path handling and validation\n- Integration with existing chat system\n\nResearch tasks:\n1. Review cursor-agent's image_tools.py implementation\n2. Check LangChain's multimodal support for each provider\n3. Design tool API for image analysis\n4. Consider TUI integration for displaying/referencing images\n5. Evaluate use cases (code screenshots, diagrams, UI mockups)\n6. Security considerations (file validation, size limits)\n\nSuccess criteria:\n- Document LangChain multimodal capabilities per provider\n- Create API design for image analysis tool\n- Design TUI workflow for image handling\n- Identify security requirements\n- Estimate implementation complexity\n\nReferences:\n- cursor-agent image_tools.py\n- LangChain multimodal docs\n- Consoul providers: src/consoul/ai/providers/",
  "status": "done",
  "type": "task",
  "priority": "low",
  "labels": [],
  "assignee": "jared@goatbytes.io",
  "reporter": "jared@goatbytes.io",
  "blocked_by": [],
  "blocks": [],
  "comments": [
    {
      "created_at": "2025-11-15T00:42:05.572202",
      "updated_at": "2025-11-15T00:42:05.572203",
      "id": "20251115004205-1fc3bf9b",
      "ticket_id": "SOUL-92",
      "author": "jared@goatbytes.io",
      "content": "# Research Findings: Image Analysis Tools for Multimodal Capabilities\n\n## 1. cursor-agent Implementation Analysis\n\n### Architecture\n- **Tool**: `query_images` - Async function accepting query + image paths\n- **Permission System**: Uses PermissionManager for read_image requests\n- **Validation**: File existence, extension checking (.jpg, .jpeg, .png, .gif, .webp, .bmp, .tiff)\n- **Agent Integration**: Delegates to agent.query_image() method\n\n### Key Features\n- Multi-image support (List[str] of paths)\n- Permission-gated access with detailed logging\n- Clear error propagation in dict response format\n- Decoupled from specific LLM provider\n\n## 2. LangChain Multimodal Support Research\n\n### General Capabilities\nLangChain supports three image input methods across providers:\n1. **URL-based**: Direct HTTP/HTTPS image URLs\n2. **Base64-encoded**: Inline image data with MIME type\n3. **Provider file IDs**: Managed file references (e.g., Anthropic Files API)\n\n### Provider-Specific Support\n\n#### Claude (Anthropic)\n- **Models**: Claude 3+ (Opus, Sonnet, Haiku), Claude 3.5 Sonnet\n- **Formats**: JPEG, PNG, GIF, WebP\n- **Implementation**:\n  ```python\n  from langchain_anthropic import ChatAnthropic\n  \n  model = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\n  message = {\n      \"role\": \"user\",\n      \"content\": [\n          {\"type\": \"text\", \"text\": \"Describe this image\"},\n          {\n              \"type\": \"image\",\n              \"source\": {\n                  \"type\": \"base64\",\n                  \"media_type\": \"image/jpeg\",\n                  \"data\": base64_data\n              }\n          }\n      ]\n  }\n  ```\n- **Files API**: Supports uploading images for reuse (beta feature with betas=[\"files-api-2025-04-14\"])\n\n#### OpenAI (GPT-4V)\n- **Models**: gpt-4-vision-preview, gpt-4o, gpt-4o-mini\n- **Formats**: JPEG, PNG, GIF, WebP\n- **Implementation**:\n  ```python\n  from langchain_openai import ChatOpenAI\n  \n  model = ChatOpenAI(model=\"gpt-4o\")\n  message = HumanMessage(content=[\n      {\"type\": \"text\", \"text\": \"What's in this image?\"},\n      {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img}\"}}\n  ])\n  ```\n\n#### Google Gemini\n- **Models**: gemini-pro-vision, gemini-2.0-flash, gemini-1.5-pro\n- **Formats**: JPEG, PNG, WebP (+ audio, video for 2.0+)\n- **Limitations**: Maximum 1 \"human\" message when using images\n- **Implementation**:\n  ```python\n  from langchain_google_genai import ChatGoogleGenerativeAI\n  \n  model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n  # Similar content structure to OpenAI\n  ```\n\n#### Ollama\n- **Models**: llava, bakllava, llava-phi3\n- **Note**: Local vision models with varying capabilities\n\n## 3. Proposed Tool API Design\n\n### Tool Schema\n```python\nfrom langchain_core.tools import tool\nfrom pydantic import BaseModel, Field\n\nclass AnalyzeImagesInput(BaseModel):\n    \"\"\"Input for analyze_images tool.\"\"\"\n    query: str = Field(description=\"Question or instruction about the image(s)\")\n    image_paths: list[str] = Field(\n        description=\"List of local file paths to images to analyze\",\n        min_length=1\n    )\n\n@tool(args_schema=AnalyzeImagesInput)\nasync def analyze_images(query: str, image_paths: list[str]) -> str:\n    \"\"\"Analyze one or more images using the AI model's vision capabilities.\n    \n    Supports: screenshots, diagrams, UI mockups, code screenshots, charts, etc.\n    Formats: JPEG, PNG, GIF, WebP\n    \"\"\"\n    ...\n```\n\n### Configuration Model\n```python\nclass ImageAnalysisToolConfig(BaseModel):\n    \"\"\"Configuration for image analysis tool.\"\"\"\n    enabled: bool = False  # Disabled by default (requires vision-capable model)\n    max_images_per_query: int = 5\n    max_image_size_mb: float = 5.0\n    allowed_extensions: list[str] = [\".jpg\", \".jpeg\", \".png\", \".gif\", \".webp\"]\n    blocked_paths: list[str] = [\"/etc\", \"/sys\", \"~/.ssh\", \"~/.aws\"]\n```\n\n## 4. TUI Integration Design\n\n### Message Display\n- **Image References**: Show thumbnail/icon + file path in message bubble\n- **Rich Integration**: Use Rich's Image protocol for terminal image preview (where supported)\n- **Fallback**: Display \"[Image: /path/to/image.png]\" placeholder\n\n### User Workflow\n1. User sends message with image reference (e.g., \"analyze screenshot.png\")\n2. TUI detects image path in message\n3. Tool call initiated with image_paths parameter\n4. Response displayed with visual context\n\n### Input Handling\n```python\n# Example user inputs:\n\"What's wrong with this code? [screenshot.png]\"\n\"Compare these two designs: ui_v1.png ui_v2.png\"\n\"/analyze screenshot.png - explain the error\"\n```\n\n## 5. Security Considerations\n\n### File Validation\n- **Extension checking**: Strict whitelist (JPEG, PNG, GIF, WebP)\n- **Magic byte verification**: Validate actual file type vs extension\n- **Size limits**: Max 5MB per image (configurable)\n- **Path traversal protection**: Block \"..\" and resolve absolute paths\n- **Blocked directories**: /etc, /sys, ~/.ssh, ~/.aws, etc.\n\n### Permission System\n- **Risk Level**: CAUTION (reads user files, sends to external API)\n- **Approval Flow**: Require user confirmation for image uploads\n- **Privacy Warning**: Inform user images sent to LLM provider\n- **Audit Logging**: Track which images were analyzed\n\n### Data Handling\n- **Base64 Encoding**: Encode images locally before API call\n- **No Persistence**: Don't cache/store encoded images\n- **Error Handling**: Clear error messages for file access issues\n- **Rate Limiting**: Respect provider API limits\n\n## 6. Implementation Complexity Estimate\n\n### Phase 1: Core Tool (3-5 days) - PRIORITY\n- [ ] Create src/consoul/ai/tools/implementations/analyze_images.py\n- [ ] Add ImageAnalysisToolConfig to config/models.py\n- [ ] Implement file validation + base64 encoding\n- [ ] Add tool registration in registry.py\n- [ ] Unit tests for validation logic\n\n### Phase 2: Provider Integration (2-3 days)\n- [ ] Detect vision capability per provider/model\n- [ ] Create multimodal message formatting helper\n- [ ] Add provider-specific message construction\n- [ ] Integration tests with mocked models\n\n### Phase 3: TUI Integration (3-4 days)\n- [ ] Parse image references from user messages\n- [ ] Display image placeholders in chat\n- [ ] Add Rich terminal image preview (optional)\n- [ ] Update message rendering for multimodal content\n\n### Phase 4: Security & Polish (2-3 days)\n- [ ] Implement magic byte validation\n- [ ] Add permission approval UI\n- [ ] Privacy warnings + audit logging\n- [ ] Documentation + examples\n\n**Total Estimate: 10-15 days**\n\n## 7. Use Cases\n\n### Code Understanding\n- \"What's the error in this screenshot?\"\n- \"Explain the flow in this diagram\"\n- \"Review this UI mockup for accessibility\"\n\n### Debugging\n- \"Why is my terminal showing this error? [terminal.png]\"\n- \"Compare these two test outputs\"\n\n### Design Review\n- \"Is this color scheme accessible?\"\n- \"Suggest improvements for this interface\"\n\n### Documentation\n- \"Generate alt text for this diagram\"\n- \"Describe what's happening in this animation frame\"\n\n## 8. Risks & Mitigations\n\n### Risk: Privacy Leaks\n**Mitigation**: Clear warnings, approval prompts, blocked paths for sensitive dirs\n\n### Risk: Large Image Files\n**Mitigation**: 5MB size limit, compression recommendations in error messages\n\n### Risk: Provider API Costs\n**Mitigation**: User confirmation, audit trail, rate limiting\n\n### Risk: Unsupported Models\n**Mitigation**: Feature detection, graceful degradation with clear error messages\n\n## 9. Success Criteria\n\n\u2705 Research complete with provider-specific documentation\n\u2705 Tool API designed following Consoul patterns\n\u2705 TUI workflow conceptualized\n\u2705 Security requirements identified\n\u2705 Implementation complexity estimated",
      "edited": false,
      "edit_count": 0,
      "is_ai_generated": false,
      "attachments": [],
      "attachment_count": 0
    },
    {
      "created_at": "2025-11-29T15:34:04.599041",
      "updated_at": "2025-11-29T15:34:04.599042",
      "id": "20251129153404-1584be38",
      "ticket_id": "SOUL-92",
      "author": "jared@goatbytes.io",
      "content": "Code review completed. Comprehensive research on image analysis tools successfully documented:\n\n\u2705 Research Completed:\n- cursor-agent implementation analyzed (query_images tool pattern)\n- LangChain multimodal support documented for all providers\n- API design proposed with AnalyzeImagesInput schema\n- TUI integration workflow designed\n- Security requirements identified (file validation, permissions, privacy)\n- Implementation complexity estimated (10-15 days)\n\n\u2705 Provider Coverage:\n- Claude 3+ (Anthropic): JPEG, PNG, GIF, WebP, Files API support\n- GPT-4V (OpenAI): Vision models with multimodal content\n- Gemini (Google): Vision + audio/video for 2.0+\n- Ollama: Local vision models (llava, bakllava)\n\n\u2705 Design Artifacts:\n- Tool schema with AnalyzeImagesInput model\n- ImageAnalysisToolConfig configuration\n- TUI message display patterns\n- User workflow examples\n- Security validation requirements\n\n\u2705 Implementation Plan:\n- Phase 1: Core tool (3-5 days)\n- Phase 2: Provider integration (2-3 days)\n- Phase 3: TUI integration (3-4 days)\n- Phase 4: Security & polish (2-3 days)\n\nAll research objectives met. Ready to create implementation tickets (SOUL-112-119). Research ticket complete.",
      "edited": false,
      "edit_count": 0,
      "is_ai_generated": false,
      "attachments": [],
      "attachment_count": 0
    }
  ],
  "attachments": [],
  "attachment_count": 0,
  "comment_count": 2,
  "order": 0,
  "custom_fields": {},
  "_archive_metadata": {
    "archived_at": "2025-12-03T15:09:13.194160",
    "archived_from_status": "done",
    "archived_by": "gira-cli"
  }
}