{
  "created_at": "2025-12-15T16:47:05.957110",
  "updated_at": "2025-12-15T16:47:05.957125",
  "id": "SOUL-280",
  "uuid": "82a5d80c-435b-4582-b104-cb380aae8220",
  "title": "Improve SDK service test coverage to 80%+",
  "description": "**As a** Consoul developer\n**I want** SDK service layer test coverage at 80% or higher\n**So that** critical business logic is properly tested and regressions are prevented\n\n**Context:**\nEPIC-012 review identified test coverage gaps in SDK services:\n- ConversationService: 35.66% (target: 80%+)\n- ModelService: 46.64% (target: 80%+)\n- ToolService: 81.82% ✅ (already exceeds target)\n\nWhile comprehensive tests exist (57 passing tests covering main flows), coverage percentage is below the 80% target. After SOUL-279 fixes the async streaming issue, additional tests should be added to cover edge cases and error paths.\n\n**Current Test Coverage:**\n- Total project: 14.8% (overall codebase)\n- ToolService: 81.82% ✅ (exceeds target)\n- ModelService: 46.64% (needs +33%)\n- ConversationService: 35.66% (needs +44%)\n\n**Technical Notes:**\n- Tests location: tests/sdk/services/\n- Current tests: test_conversation_service.py (23 failing, needs SOUL-279 first)\n- Current tests: test_model_service.py, test_tool_service.py (passing)\n- Dependencies: SOUL-279 must complete first (fix async streaming)\n- Use pytest --cov=src/consoul/sdk to measure coverage\n\n**Acceptance Criteria:**\n\n**Given** SOUL-279 async streaming fix is complete\n**When** Running pytest --cov=src/consoul/sdk\n**Then** ConversationService coverage is 80%+\n\n**Given** SOUL-279 async streaming fix is complete\n**When** Running pytest --cov=src/consoul/sdk\n**Then** ModelService coverage is 80%+\n\n**Given** All service tests\n**When** Running test suite\n**Then** All tests pass (100% pass rate)\n\n**Given** Edge case in service code\n**When** Test coverage analysis\n**Then** Edge case has corresponding test\n\n**Testing Considerations:**\n- Add tests for error paths (API failures, timeouts, rate limits)\n- Add tests for edge cases (empty messages, None values, invalid inputs)\n- Add tests for multimodal message handling\n- Add tests for context window trimming edge cases\n- Add tests for cost calculation accuracy\n- Add tests for tool execution error handling\n- Mock all external dependencies (LangChain models, databases)\n- Use pytest-asyncio for async test methods\n- Parametrize tests for multiple providers where applicable\n\n**Implementation Hints:**\n**ConversationService gaps to cover:**\n- Error handling in _stream_response()\n- Edge cases in _get_trimmed_messages() (no config, various max_tokens)\n- Multimodal message edge cases (_has_multimodal_content)\n- Cost calculation edge cases (missing metadata, None values)\n- Tool execution error paths (_execute_tool_calls failures)\n- Message reconstruction edge cases (_reconstruct_message)\n- Attachment handling errors (invalid files, missing files)\n\n**ModelService gaps to cover:**\n- list_ollama_models() with Ollama not running\n- list_mlx_models() on non-Mac systems\n- switch_model() with invalid model names\n- Provider detection edge cases\n- Model capability queries (supports_vision, supports_tools)\n- Pricing lookup for unknown models\n- API key validation errors\n\n**Test Organization:**\n- Group by feature area (streaming, tools, stats, etc.)\n- Use descriptive test class names (TestStreamingResponse, TestToolExecution)\n- Follow existing test patterns from test_conversation_service.py\n- Add docstrings explaining what each test validates\n- Use fixtures for common setup (mock models, mock configs)",
  "status": "backlog",
  "type": "task",
  "priority": "high",
  "labels": [
    "testing",
    "sdk",
    "quality"
  ],
  "reporter": "jared@goatbytes.io",
  "epic_id": "EPIC-012",
  "blocked_by": [],
  "blocks": [],
  "comments": [],
  "attachments": [],
  "attachment_count": 0,
  "comment_count": 0,
  "story_points": 8,
  "order": 0,
  "custom_fields": {}
}