{
  "created_at": "2025-11-08T16:29:11.843308",
  "updated_at": "2025-11-09T21:08:28.717051Z",
  "id": "SOUL-25",
  "uuid": "15c4deb8-22b3-4d84-9049-136201deb40d",
  "title": "Add streaming response handling",
  "description": "## User Story\nAs a Consoul user, I want to see AI responses stream in real-time so that I get immediate feedback and can interrupt long responses.\n\n## Description\nImplement streaming response handling for all AI providers using LangChain's `.stream()` method. Provide token-by-token display with progress indicators, interrupt handling (Ctrl+C), and smooth terminal output using Rich console features.\n\n## Technical Notes\n- **File**: `src/consoul/ai/streaming.py`\n- **LangChain Docs**: https://python.langchain.com/docs/concepts/streaming/\n- **Dependencies**: rich (already used in Consoul)\n- Use `model.stream(messages)` for token-by-token output\n- Handle keyboard interrupts gracefully\n- Display progress indicators during streaming\n- Collect full response for history/persistence\n- Support all providers (OpenAI, Anthropic, Google, Ollama)\n\n## Acceptance Criteria\n- **Given** a streaming-enabled model\n- **When** generating a response\n- **Then** tokens are displayed as they arrive\n\n- **Given** a long-running stream\n- **When** user presses Ctrl+C\n- **Then** streaming stops gracefully and partial response is saved\n\n- **Given** streaming output\n- **When** tokens arrive\n- **Then** a progress indicator shows the model is working\n\n- **Given** streaming completes\n- **When** the full response is received\n- **Then** the complete message is available for history\n\n- **Given** streaming fails\n- **When** an error occurs mid-stream\n- **Then** error is displayed and partial response is preserved\n\n## Testing Considerations\n- Mock streaming responses with delayed token generation\n- Test interrupt handling (simulate Ctrl+C)\n- Verify progress indicator display\n- Test error handling during streaming\n- Test with all provider implementations\n- Verify terminal output formatting\n\n## Implementation Hints\n```python\nfrom rich.console import Console\nfrom rich.live import Live\nfrom rich.spinner import Spinner\n\nconsole = Console()\n\nasync def stream_response(model, messages):\n    \"\"\"Stream response with progress indicator.\"\"\"\n    collected_tokens = []\n    \n    try:\n        with Live(Spinner(\"dots\"), console=console) as live:\n            for chunk in model.stream(messages):\n                token = chunk.content\n                collected_tokens.append(token)\n                \n                # Update display\n                live.update(\n                    \"\".join(collected_tokens),\n                    refresh=True\n                )\n    except KeyboardInterrupt:\n        console.print(\"\\n[yellow]Interrupted[/yellow]\")\n    \n    return \"\".join(collected_tokens)\n```\n\n## Dependencies\n- SOUL-20 (provider initialization)\n- SOUL-21 (OpenAI support)\n- SOUL-22 (Anthropic support)\n- SOUL-23 (Google support)\n- SOUL-24 (Ollama support)\n\n## Story Points\n5 - Moderate complexity with interrupt handling and terminal UX",
  "status": "done",
  "type": "feature",
  "priority": "high",
  "labels": [
    "ai",
    "streaming",
    "ux"
  ],
  "reporter": "jared@goatbytes.io",
  "epic_id": "EPIC-002",
  "blocked_by": [],
  "blocks": [],
  "comments": [],
  "attachments": [],
  "attachment_count": 0,
  "comment_count": 0,
  "story_points": 5,
  "order": 0,
  "custom_fields": {}
}