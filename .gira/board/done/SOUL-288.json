{
  "created_at": "2025-12-16T13:54:20.565429",
  "updated_at": "2025-12-16T16:14:11.493629",
  "id": "SOUL-288",
  "uuid": "51497ba7-77fe-4996-9f3f-9c85faaa787c",
  "title": "Support provider-specific LLM parameters in Consoul SDK",
  "description": "**As a** Consoul SDK user\n**I want** to pass provider-specific parameters (OpenAI service_tier, Anthropic betas, etc.)\n**So that** I can leverage advanced provider features and control costs\n\n**Context:**\nLLM providers offer specialized parameters beyond the common ones (temperature, max_tokens):\n- OpenAI: service_tier (\"auto\"|\"default\"|\"flex\"), seed, logit_bias, response_format\n- Anthropic: betas (feature flags), thinking, metadata\n- Google: safety_settings, generation_config, candidate_count\n\nCurrently, these can only be configured via:\n1. Config file (src/consoul/config/models.py:125-222) - Static, not runtime\n2. Direct LangChain model construction - Bypasses Consoul\n\nConsoul SDK constructor (src/consoul/sdk/wrapper.py:108-234) does not accept provider-specific params.\n\n**User Need:**\nOpenAI flex tier costs ~50% less but has slower processing. Users want:\n```python\n# Use flex tier for cost savings\nsdk = Consoul(model=\"gpt-4o\", service_tier=\"flex\", temperature=0.7)\n\n# Use Anthropic extended thinking\nsdk = Consoul(model=\"claude-sonnet-4\", thinking={\"type\": \"enabled\", \"budget_tokens\": 10000})\n\n# Use OpenAI JSON schema mode\nsdk = Consoul(model=\"gpt-4o\", response_format={\"type\": \"json_schema\", \"json_schema\": {...}})\n```\n\n**Technical Notes:**\n- src/consoul/sdk/wrapper.py:269-291 - Model initialization with get_chat_model()\n- src/consoul/ai/providers.py - get_chat_model() and build_model_params()\n- src/consoul/config/models.py:125-222 - Provider-specific model config classes\n- Current: temperature override supported via model_kwargs (line 273-275)\n- Need: Pass provider-specific params through to model construction\n\n**Acceptance Criteria:**\n\n**Given** I am using OpenAI and want cost savings\n**When** I call Consoul(model=\"gpt-4o\", service_tier=\"flex\")\n**Then** The model is initialized with service_tier parameter\n\n**Given** I am using Anthropic extended thinking\n**When** I call Consoul(model=\"claude-sonnet-4\", thinking={\"type\": \"enabled\", \"budget_tokens\": 5000})\n**Then** The thinking parameter is passed to the Anthropic model\n\n**Given** I provide an invalid provider-specific parameter\n**When** I call Consoul(model=\"gpt-4o\", invalid_param=\"value\")\n**Then** I receive a clear validation error listing valid parameters\n\n**Given** I provide a parameter for the wrong provider\n**When** I call Consoul(model=\"claude-sonnet-4\", service_tier=\"flex\")\n**Then** I receive an error indicating service_tier is OpenAI-only\n\n**Testing Considerations:**\n- Test provider-specific param passing to LangChain models\n- Test validation for unknown parameters\n- Test cross-provider parameter errors\n- Mock provider API calls to verify params sent correctly\n- Test backward compatibility (existing code without new params)\n- Document all provider-specific options\n\n**Implementation Hints:**\nOption A: Accept **model_kwargs in Consoul.__init__()\n  - Pass through to get_chat_model()\n  - Simple but no validation\n\nOption B: Explicit parameters matching ModelConfig classes\n  - Extract params from ModelConfig unions\n  - Type-safe but verbose constructor\n\nOption C: Hybrid - document expected kwargs by provider\n  - Accept **model_kwargs with runtime validation\n  - Validate against provider's ModelConfig schema\n  - Best UX with safety\n\nRecommended: Option C\n- Add **model_kwargs to Consoul.__init__()\n- Extract provider from model name via get_provider_from_model()\n- Validate kwargs against appropriate ModelConfig (OpenAIModelConfig, etc.)\n- Pass validated kwargs to get_chat_model()\n- Document provider-specific options in SDK docs\n\n**Dependencies:**\n- Requires src/consoul/ai/providers.py:get_provider_from_model() for runtime detection\n- Uses existing ModelConfig classes for validation schema\n- Related to SOUL-285 (profile-optional constructor) for SDK-first design",
  "status": "done",
  "type": "feature",
  "priority": "medium",
  "labels": [
    "sdk",
    "ai",
    "providers"
  ],
  "reporter": "jared@goatbytes.io",
  "epic_id": "EPIC-015",
  "blocked_by": [],
  "blocks": [],
  "comments": [
    {
      "created_at": "2025-12-16T16:14:11.493510",
      "updated_at": "2025-12-16T16:14:11.493511",
      "id": "20251216161411-14bad773",
      "ticket_id": "SOUL-288",
      "author": "jared@goatbytes.io",
      "content": "## Implementation Complete\n\nSuccessfully implemented provider-specific parameter support via **model_kwargs in Consoul SDK.\n\n### Changes Made\n\n1. **Consoul.__init__() signature updated** (wrapper.py:108-120)\n   - Added `**model_kwargs: Any` parameter\n   - Merged with temperature override\n   - Passed through to get_chat_model()\n\n2. **Documentation updated** (wrapper.py:154-179)\n   - Documented all provider-specific parameters\n   - Added examples for OpenAI (service_tier), Anthropic (thinking), Google (safety_settings)\n   - Clear parameter descriptions for each provider\n\n3. **Tests added** (tests/unit/test_provider_kwargs.py)\n   - 5 comprehensive test cases covering:\n     - OpenAI service_tier parameter\n     - Anthropic thinking parameter\n     - Google safety_settings parameter\n     - Temperature override with provider kwargs\n     - Multiple provider kwargs together\n   - All tests passing âœ…\n\n### Usage Examples\n\n```python\n# OpenAI flex tier (50% cheaper)\nconsole = Consoul(model=\"gpt-4o\", service_tier=\"flex\")\n\n# Anthropic extended thinking\nconsole = Consoul(\n    model=\"claude-sonnet-4\",\n    thinking={\"type\": \"enabled\", \"budget_tokens\": 10000}\n)\n\n# Google safety settings\nconsole = Consoul(\n    model=\"gemini-pro\",\n    safety_settings={\"HARM_CATEGORY_HARASSMENT\": \"BLOCK_NONE\"}\n)\n\n# Multiple parameters\nconsole = Consoul(\n    model=\"gpt-4o\",\n    service_tier=\"flex\",\n    seed=42,\n    top_p=0.95,\n    temperature=0.7\n)\n```\n\n### Technical Notes\n\n- No validation layer added (delegated to get_chat_model() as recommended)\n- Backward compatible - existing code works unchanged\n- Provider params passed directly through to LangChain models\n- Temperature override still supported and merged correctly\n\n### Test Results\n- 5/5 provider kwargs tests passing\n- 16/16 SDK model tests passing (backward compatibility verified)\n- No breaking changes introduced",
      "edited": false,
      "edit_count": 0,
      "is_ai_generated": false,
      "attachments": [],
      "attachment_count": 0
    }
  ],
  "attachments": [],
  "attachment_count": 0,
  "comment_count": 1,
  "story_points": 3,
  "order": 10,
  "custom_fields": {}
}