{
  "created_at": "2025-12-30T02:05:53.014890Z",
  "updated_at": "2025-12-31T02:13:14.736128Z",
  "id": "SOUL-329",
  "uuid": "e5968697-ba74-4794-b869-7f69bbc44a17",
  "_version": 1,
  "title": "Add Grafana dashboard templates for Prometheus metrics",
  "description": "**As a** Consoul operator deploying to production\n**I want** pre-built Grafana dashboard templates for Consoul metrics\n**So that** I can quickly visualize API health, performance, and token usage without manual dashboard creation\n\n**Context:**\nThe Consoul server exposes Prometheus metrics via `src/consoul/server/observability/metrics.py` on a separate port (default: 9090). The operations runbook (`docs/operations/runbook.md`) documents these metrics and PromQL queries, and alert rules are defined in the runbook. However, operators must currently create Grafana dashboards manually, which is time-consuming and error-prone.\n\nLegal industry deployments (EPIC-018) require production-ready monitoring out of the box. Pre-built dashboards reduce onboarding time and ensure consistent monitoring across deployments.\n\n**Available Metrics (from metrics.py lines 111-140):**\n\n| Metric | Type | Labels |\n|--------|------|--------|\n| `consoul_request_total` | Counter | endpoint, method, status, model |\n| `consoul_request_latency_seconds` | Histogram | endpoint, method |\n| `consoul_token_usage_total` | Counter | direction, model, session_id |\n| `consoul_active_sessions` | Gauge | - |\n| `consoul_tool_executions_total` | Counter | tool_name, status |\n| `consoul_errors_total` | Counter | endpoint, error_type |\n\n**Histogram Buckets (lines 120-121):**\n`(0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0, 30.0, 60.0)`\n\n**Technical Notes:**\n- Create directory: `deployment/monitoring/grafana/`\n- Create dashboard JSON: `deployment/monitoring/grafana/consoul-dashboard.json`\n- Create README: `deployment/monitoring/grafana/README.md`\n- Dashboard format: Grafana JSON model (exportable via Grafana UI)\n- Use Grafana dashboard schema version 38+ (Grafana 10.x compatible)\n- Include template variables for filtering (endpoint, model)\n- Reference existing PromQL queries from `docs/operations/runbook.md` (lines 185-220)\n- Match alert thresholds from runbook (lines 300-306): error rate > 1%/5%, latency > 2s/5s\n\n**Dashboard Panels (Required):**\n\n1. **Overview Row:**\n   - Request Rate (requests/second) - Stat panel\n   - Error Rate (percentage) - Stat panel with thresholds (green <1%, yellow <5%, red >=5%)\n   - Active Sessions - Stat panel\n   - p95 Latency - Stat panel\n\n2. **Request Metrics Row:**\n   - Request Rate by Endpoint - Time series, stacked\n   - Request Rate by Status Code - Time series (200, 4xx, 5xx)\n   - Request Rate by Model - Time series\n\n3. **Latency Row:**\n   - p50/p95/p99 Latency Over Time - Time series with 3 lines\n   - Latency Heatmap - Heatmap using histogram buckets\n   - Latency by Endpoint - Time series\n\n4. **Token Usage Row:**\n   - Token Usage Rate (input vs output) - Time series\n   - Token Usage by Model - Time series\n   - Cumulative Token Count - Stat panel\n\n5. **Tool Executions Row:**\n   - Tool Execution Rate - Time series by tool_name\n   - Tool Success Rate - Gauge (percentage)\n   - Tool Execution Table - Table with tool_name, success, failure counts\n\n6. **Errors Row:**\n   - Error Rate by Type - Time series (validation, internal, timeout, etc.)\n   - Error Rate by Endpoint - Time series\n   - Recent Errors Table - Table with endpoint, error_type, count\n\n**Template Variables:**\n- `$endpoint` - Multi-select from `consoul_request_total` endpoint label\n- `$model` - Multi-select from `consoul_request_total` model label\n- `$interval` - Auto interval for aggregation\n\n**Alert Annotations:**\n- Vertical annotation lines when alerts fire (ConsoulHighErrorRate, ConsoulHighLatency)\n- Requires Grafana alerting or external Alertmanager\n\n**Acceptance Criteria:**\n\n**Given** the dashboard JSON file exists at `deployment/monitoring/grafana/consoul-dashboard.json`\n**When** I import it into Grafana (via UI or API)\n**Then** all 6 panels rows render correctly with sample data\n\n**Given** Consoul is running with Prometheus metrics enabled\n**When** I connect Grafana to Prometheus and load the dashboard\n**Then** I see real-time metrics updating every 15 seconds\n\n**Given** the template variables are defined\n**When** I select a specific endpoint or model\n**Then** all panels filter to show only that endpoint/model\n\n**Given** the error rate exceeds 5%\n**When** viewing the Error Rate stat panel\n**Then** the panel background is red (threshold visualization)\n\n**Given** I want to deploy the dashboard\n**When** I follow the README instructions\n**Then** I can import via Grafana UI, Grafana API, or Kubernetes ConfigMap\n\n**Testing Considerations:**\n- Validate JSON syntax (jsonlint or jq)\n- Test import into Grafana 10.x and 11.x\n- Verify all PromQL queries execute without errors\n- Test with mock Prometheus data (prometheus-test-data generator)\n- Verify template variable queries work\n- Check responsive layout on different screen sizes\n- Test threshold colors render correctly\n\n**Implementation Hints:**\n- Use Grafana's \"Export as JSON\" after building manually, or use Grafonnet/dashboard-as-code\n- Reference existing PromQL from `docs/operations/runbook.md`:\n  - Request rate: `sum(rate(consoul_request_total[5m]))`\n  - Error rate: `sum(rate(consoul_errors_total[5m])) / sum(rate(consoul_request_total[5m])) * 100`\n  - p95 latency: `histogram_quantile(0.95, sum(rate(consoul_request_latency_seconds_bucket[5m])) by (le))`\n- Grafana JSON documentation: https://grafana.com/docs/grafana/latest/dashboards/build-dashboards/view-dashboard-json-model/\n- Use `\"datasource\": {\"type\": \"prometheus\", \"uid\": \"${DS_PROMETHEUS}\"}` for portable datasource references\n- Include `__inputs` section for datasource provisioning\n- Add time range defaults: last 1 hour, with quick ranges\n- Set refresh interval: 15s for real-time, 1m for production\n- Reference: SOUL-299 (observability integration), SOUL-302 (operations runbook)",
  "status": "done",
  "type": "feature",
  "priority": "medium",
  "labels": [
    "observability",
    "monitoring",
    "documentation",
    "grafana"
  ],
  "assignee": "jared@goatbytes.io",
  "reporter": "jared@goatbytes.io",
  "epic_id": "EPIC-018",
  "blocked_by": [],
  "blocks": [],
  "comments": [],
  "attachments": [],
  "attachment_count": 0,
  "comment_count": 0,
  "story_points": 3,
  "order": 0,
  "custom_fields": {}
}