{
  "created_at": "2025-12-06T09:42:46.488519",
  "updated_at": "2025-12-06T17:52:42.998453Z",
  "id": "SOUL-231",
  "uuid": "7076e2b2-8ef7-4026-964e-45bda061cc2c",
  "title": "Extract and cache ChatLlamaCpp context size at runtime",
  "description": "**As a** Consoul user with local GGUF models (ChatLlamaCpp)\n**I want** consoul to automatically detect the actual context window size\n**So that** I don't get context overflow errors or unnecessarily conservative trimming\n\n**Context:**\nCurrently, ChatLlamaCpp models (GGUF files) fall back to a conservative 4096 token limit because get_model_token_limit() cannot programmatically determine the actual context size. This causes two problems:\n\n1. Context overflow errors: If system prompt + user message > 4096 tokens but model actually has 8K+ context\n2. Over-trimming: Messages get trimmed unnecessarily when model could handle more\n\nReal-world example:\n- User has NSFW-3B GGUF model with 8K context configured in LMStudio\n- System prompt is ~19KB chars (~5K tokens)\n- Consoul assumes 4096 token limit resulting in ERROR: Requested tokens (5180) exceed context window of 4096\n- Model could actually handle it!\n\n**Current Implementation:**\n- src/consoul/ai/providers.py:1280 - ChatLlamaCpp initialized with n_ctx from config (default 4096)\n- src/consoul/ai/context.py:267 - Falls back to DEFAULT_TOKEN_LIMIT when model path not recognized\n- No mechanism to extract actual n_ctx from loaded ChatLlamaCpp instance\n\n**Research Findings:**\n- ChatLlamaCpp stores n_ctx internally but does not expose it via public API\n- LangChain ChatLlamaCpp wraps llama-cpp-python Llama class\n- The underlying Llama client likely has n_ctx accessible\n- Ollama has similar pattern using API query (_get_ollama_context_length)\n\n**Technical Notes:**\n- ChatLlamaCpp initialization: providers.py:1280\n- Token limit detection: context.py:204-267\n- Model path patterns for GGUF: File paths like /Users/.../model.gguf\n- Cache mechanism exists: ~/.consoul/ollama_context_cache.json (context.py:127-149)\n- Could extend cache to support llamacpp models too\n\n**Acceptance Criteria:**\n\nGiven I initialize a ChatLlamaCpp model with n_ctx=8192\nWhen the model is loaded\nThen consoul extracts the actual 8192 context size and uses it for trimming\n\nGiven I have a GGUF model with known context size\nWhen I use it across multiple sessions\nThen the context size is cached and reused without re-detection\n\nGiven extraction fails (e.g., llama-cpp-python API change)\nWhen context size cannot be determined\nThen fall back to 4096 with a clear warning log message\n\nGiven I configure n_ctx=16384 in my config\nWhen I load the model\nThen consoul respects that configured value and caches it\n\n**Testing Considerations:**\n- Mock ChatLlamaCpp instance to test n_ctx extraction\n- Test cache read/write for GGUF model paths\n- Test fallback when extraction fails\n- Integration test with actual llama-cpp-python (optional dependency)\n- Verify warning logs when using fallback\n- Test with various n_ctx values (4096, 8192, 16384, 32768)\n\n**Implementation Hints:**\n1. After ChatLlamaCpp initialization extract n_ctx from the configured value passed to __init__\n2. Cache the context size in ollama_context_cache.json with model path as key\n3. Update get_model_token_limit() to check cache for GGUF paths before DEFAULT_TOKEN_LIMIT\n4. Add _get_llamacpp_context_length() similar to _get_ollama_context_length()\n5. Handle edge cases: relative vs absolute paths, symlinks\n\n**Files to Modify:**\n- src/consoul/ai/providers.py - Cache n_ctx after ChatLlamaCpp init\n- src/consoul/ai/context.py - Add llamacpp cache lookup\n- tests/ai/test_context.py - Add llamacpp extraction tests\n\n**References:**\n- Similar issue: Context overflow with local models (SOUL-228)",
  "status": "in_progress",
  "type": "feature",
  "priority": "medium",
  "labels": [
    "ai",
    "providers",
    "llamacpp",
    "ux-improvement"
  ],
  "reporter": "jared@goatbytes.io",
  "blocked_by": [],
  "blocks": [],
  "comments": [],
  "attachments": [],
  "attachment_count": 0,
  "comment_count": 0,
  "story_points": 3,
  "order": 0,
  "custom_fields": {}
}