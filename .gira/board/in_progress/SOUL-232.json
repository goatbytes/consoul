{
  "created_at": "2025-12-07T11:46:48.022099",
  "updated_at": "2025-12-19T21:13:55.539285",
  "id": "SOUL-232",
  "uuid": "cc127062-5acc-477b-8bcc-cb4ea2e2ec31",
  "title": "Integrate LiteLLM model registry for automatic token limit updates",
  "description": "**As a** Consoul maintainer\n**I want** to automatically fetch model token limits from a community-maintained registry\n**So that** we don't need manual code updates for every new model release\n\n**Context:**\nChatGPT conversation (.local/convo.txt) confirms there's no official API to get context window sizes from any provider (OpenAI, Anthropic, Google). Our current solution uses:\n1. Hardcoded MODEL_TOKEN_LIMITS dictionary (requires manual updates)\n2. Pattern-based intelligent defaults (provides fallback for new models)\n3. Ollama API queries (automatic for Ollama models)\n4. LlamaCpp extraction (automatic for GGUF models)\n\n**Proposed Solution:**\nUse LiteLLM's community-maintained JSON file as primary source:\nhttps://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\n**Implementation Plan:**\n1. Fetch LiteLLM JSON on startup or periodically (with local cache + TTL)\n2. Merge with hardcoded MODEL_TOKEN_LIMITS (hardcoded takes precedence)\n3. Use pattern-based defaults as final fallback\n4. Add admin command to force-refresh the cache\n\n**Benefits:**\n- Automatic updates for new OpenAI/Anthropic/Google models\n- Community-maintained (1000+ contributors)\n- Includes pricing data too (useful for cost tracking)\n- Still allows local overrides via MODEL_TOKEN_LIMITS\n\n**Trade-offs:**\n- External dependency (mitigated by local cache)\n- Startup network request (mitigated by cache + lazy loading)\n- Extra complexity (minimal - ~100 LOC)\n\n**Files to Modify:**\n- src/consoul/ai/context.py - Add registry fetch/merge logic\n- Add src/consoul/ai/model_registry.py - Registry management\n- tests/ai/test_context.py - Add registry tests\n\n**References:**\n- LiteLLM JSON: https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n- ChatGPT conversation: .local/convo.txt\n- Related: Pattern-based defaults (commit c1bc176)",
  "status": "in_progress",
  "type": "feature",
  "priority": "low",
  "labels": [],
  "assignee": "jared@goatbytes.io",
  "reporter": "jared@goatbytes.io",
  "blocked_by": [],
  "blocks": [],
  "comments": [
    {
      "created_at": "2025-12-19T21:05:04.328161",
      "updated_at": "2025-12-19T21:05:04.328162",
      "id": "20251219210504-55cde650",
      "ticket_id": "SOUL-232",
      "author": "jared@goatbytes.io",
      "content": "Implementation complete with multi-tier fallback chain:\n1. Model registry (curated models with full metadata)\n2. Hardcoded MODEL_TOKEN_LIMITS dict\n3. Ollama API queries (for local models)\n4. LiteLLM community registry (auto-updates, cached 24h)\n5. Pattern-based intelligent defaults\n6. Conservative 4K fallback\n\nKey features:\n- Lazy loading (no startup performance impact)\n- Local cache with 24h TTL at ~/.consoul/litellm_cache.json\n- Graceful degradation (network failures don't break anything)\n- 100% test coverage (30 new tests)\n\nFiles created:\n- src/consoul/ai/litellm_registry.py (117 lines)\n- tests/ai/test_litellm_registry.py (22 tests)\n\nFiles modified:\n- src/consoul/registry/registry.py (added get_context_window helper)\n- src/consoul/ai/context.py (integrated full fallback chain)\n- tests/ai/test_context.py (added 8 fallback chain tests)\n\nTotal: ~250 LOC, all tests passing âœ…",
      "edited": false,
      "edit_count": 0,
      "is_ai_generated": false,
      "attachments": [],
      "attachment_count": 0
    },
    {
      "created_at": "2025-12-19T21:13:55.539183",
      "updated_at": "2025-12-19T21:13:55.539184",
      "id": "20251219211355-3744b3fb",
      "ticket_id": "SOUL-232",
      "author": "jared@goatbytes.io",
      "content": "CRITICAL BUG FIX: Resolved permanent registry disable after transient failure\n\nIssue: LiteLLM registry fetch failures were being permanently cached as empty, preventing retries for the process lifetime.\n\nRoot Cause:\n- On fetch failure, _get_registry() set _REGISTRY_CACHE = {} \n- Subsequent calls returned immediately due to non-None cache check\n- Registry never retried, defeating advertised 24h auto-update\n\nSolution:\n- Added _LAST_FETCH_ATTEMPT timestamp tracking\n- Empty results are NOT cached (returns {} without setting _REGISTRY_CACHE)\n- Retry allowed after RETRY_AFTER_FAILURE_MINUTES (5min default)\n- On success, _LAST_FETCH_ATTEMPT reset to None\n- Manual refresh_cache() clears all failure tracking\n\nImpact:\n- Transient network failures no longer permanently disable LiteLLM lookups\n- Automatic recovery after 5 minute cooldown\n- Zero breaking changes to API or behavior\n\nTests Added (3):\n- test_retry_after_transient_failure: Verifies retry after cooldown\n- test_empty_result_not_permanently_cached: Confirms no permanent caching\n- test_refresh_cache_resets_failure_tracking: Validates manual refresh\n\nCoverage: 93.94% on litellm_registry.py (25/25 tests passing)",
      "edited": false,
      "edit_count": 0,
      "is_ai_generated": false,
      "attachments": [],
      "attachment_count": 0
    }
  ],
  "attachments": [],
  "attachment_count": 0,
  "comment_count": 2,
  "order": 0,
  "custom_fields": {}
}