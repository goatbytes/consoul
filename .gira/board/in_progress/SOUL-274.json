{
  "created_at": "2025-12-13T03:31:15.702294Z",
  "updated_at": "2025-12-13T03:32:17.091539Z",
  "id": "SOUL-274",
  "uuid": "a2c834ca-cbc0-44de-b849-9c6733f3b9b1",
  "title": "Fix ModelService local provider regression - Ollama/LlamaCpp/MLX models missing",
  "description": "**As a** Consoul user with local AI models\n**I want** Ollama, LlamaCpp, and MLX models to appear in the model picker\n**So that** I can use my local models as I did before the ModelService refactoring\n\n**Problem:**\nMODEL_CATALOG in `src/consoul/sdk/catalog.py` only enumerates cloud provider models (OpenAI/Anthropic/Google/HuggingFace). Local providers (Ollama/LlamaCpp/MLX) are completely missing.\n\n**Impact:**\n- `ModelService.list_models(\"ollama\")` returns empty list\n- `ModelService.get_current_model_info()` returns None for local models\n- `ModelService.supports_vision()` returns False for local models (even vision-capable ones)\n- When TUI/CLI switches to ModelService, **all local models disappear from UI**\n- Users cannot select or use Ollama/LlamaCpp/MLX models\n\n**Root Cause:**\nDuring SOUL-247 (Create ModelService) and SOUL-268 (Use ModelService in TUI), we migrated model catalog from TUI to SDK but only included static cloud provider models. \n\nLocal models are **dynamically discovered** at runtime:\n- **Ollama**: `get_ollama_models()` fetches from running service\n- **LlamaCpp**: Scans for GGUF files in cache directories  \n- **MLX**: Scans for MLX-converted models\n- **HuggingFace**: Scans local cache for downloaded models\n\n**Current Behavior:**\n```python\nservice = ModelService.from_config(config)\nollama_models = service.list_models(\"ollama\")  # Returns []\nmlx_models = service.list_models(\"mlx\")        # Returns []\n```\n\n**Expected Behavior:**\n```python\nservice = ModelService.from_config(config)\nollama_models = service.list_models(\"ollama\")  # Returns dynamic list from Ollama\nmlx_models = service.list_models(\"mlx\")        # Returns dynamic list from MLX cache\n```\n\n**Files Affected:**\n- `src/consoul/sdk/services/model.py` - ModelService (needs dynamic discovery)\n- `src/consoul/sdk/catalog.py` - MODEL_CATALOG (static, no local models)\n\n**Existing Dynamic Discovery Functions:**\nAlready available in `src/consoul/ai/providers.py`:\n- `get_ollama_models(include_context=True)` - Fetches Ollama models\n- `is_ollama_running()` - Checks if Ollama is available\n- `get_huggingface_local_models()` - Scans HF cache\n- GGUF scanning in model picker (lines 540-600)\n\n**Solution:**\nAdd dynamic model discovery to ModelService:\n\n1. **Add discovery methods to ModelService:**\n```python\ndef _discover_ollama_models(self) -> list[ModelInfo]:\n    \"\"\"Discover Ollama models from running service.\"\"\"\n    from consoul.ai.providers import get_ollama_models, is_ollama_running\n    if not is_ollama_running():\n        return []\n    \n    models = []\n    for model_info in get_ollama_models(include_context=True):\n        model_name = model_info.get(\"name\", \"\")\n        context_length = model_info.get(\"context_length\", \"?\")\n        models.append(ModelInfo(\n            id=model_name,\n            name=model_name,\n            provider=\"ollama\",\n            context_window=self._format_context(context_length),\n            description=\"Local Ollama model\",\n            supports_vision=self._detect_vision_support(model_name),\n        ))\n    return models\n```\n\n2. **Update `list_models()` to include dynamic models:**\n```python\ndef list_models(self, provider: str | None = None) -> list[ModelInfo]:\n    from consoul.sdk.catalog import MODEL_CATALOG, get_models_by_provider\n    \n    # Get static catalog models\n    static_models = get_models_by_provider(provider) if provider else MODEL_CATALOG.copy()\n    \n    # Add dynamic local models if provider matches\n    if provider in (\"ollama\", \"llamacpp\", \"mlx\", \"huggingface\") or provider is None:\n        dynamic_models = self._discover_dynamic_models(provider)\n        static_models.extend(dynamic_models)\n    \n    return static_models\n```\n\n3. **Update `get_current_model_info()` to try dynamic discovery:**\n```python\ndef get_current_model_info(self) -> ModelInfo | None:\n    from consoul.sdk.catalog import get_model_info\n    \n    # Try static catalog first\n    info = get_model_info(self.current_model_id)\n    if info:\n        return info\n    \n    # Try dynamic discovery for local models\n    provider = self._detect_provider(self.current_model_id)\n    if provider in (\"ollama\", \"llamacpp\", \"mlx\"):\n        dynamic_models = self._discover_dynamic_models(provider)\n        return next((m for m in dynamic_models if m.id == self.current_model_id), None)\n    \n    return None\n```\n\n**Testing:**\n- Test Ollama model discovery when Ollama running\n- Test Ollama returns empty when not running\n- Test LlamaCpp GGUF discovery\n- Test MLX model discovery on macOS\n- Test HuggingFace local cache discovery\n- Test model picker shows local models\n- Test vision detection for local vision models (llava, bakllava, etc)\n\n**Acceptance Criteria:**\n**Given** Ollama is running with models\n**When** I call `service.list_models(\"ollama\")`\n**Then** I get a list of my Ollama models\n\n**Given** I'm using an Ollama model\n**When** I call `service.get_current_model_info()`\n**Then** I get ModelInfo, not None\n\n**Given** I'm using llava (vision Ollama model)\n**When** I call `service.supports_vision()`\n**Then** It returns True\n\n**Given** I open the model picker\n**When** I select the Ollama provider\n**Then** I see all my local Ollama models",
  "status": "in_progress",
  "type": "bug",
  "priority": "critical",
  "labels": [],
  "reporter": "jared@goatbytes.io",
  "blocked_by": [],
  "blocks": [],
  "comments": [],
  "attachments": [],
  "attachment_count": 0,
  "comment_count": 0,
  "order": 0,
  "custom_fields": {}
}