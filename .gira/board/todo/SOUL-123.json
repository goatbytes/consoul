{
  "created_at": "2025-11-16T08:03:22.880815",
  "updated_at": "2025-11-16T16:15:53.070252Z",
  "id": "SOUL-123",
  "uuid": "b3024b0e-48e3-4e73-9916-a8640b850233",
  "title": "Implement HuggingFace provider initialization in get_chat_model()",
  "description": "**As a** Consoul user\n**I want** to initialize and use HuggingFace models via get_chat_model()\n**So that** I can access thousands of open-source models from HuggingFace Hub\n\n**Context:**\nLangChain provides two ways to use HuggingFace models:\n1. HuggingFaceEndpoint - API-based, requires HUGGINGFACEHUB_API_TOKEN\n2. HuggingFacePipeline - Local execution, requires transformers library\n\nThis ticket implements ChatHuggingFace initialization in get_chat_model() following the established provider pattern. Start with HuggingFaceEndpoint (simpler, API-based) and optionally support local pipeline execution.\n\nPopular models to test:\n- meta-llama/Llama-3.1-8B-Instruct\n- mistralai/Mistral-7B-Instruct-v0.2\n- google/flan-t5-xxl\n\n**Technical Notes:**\n- src/consoul/ai/providers.py:342-638 - Add HuggingFace case in get_chat_model()\n- src/consoul/ai/providers.py:246-321 - Add to build_model_params()\n- Import: from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n- Wrapper pattern: ChatHuggingFace(llm=HuggingFaceEndpoint(...))\n- API key resolution: HUGGINGFACEHUB_API_TOKEN env var\n- Supports streaming via .stream() method\n- Supports tool calling (verify with supports_tool_calling())\n- Error handling: connection errors, missing token, model not found\n- pyproject.toml: Add langchain-huggingface dependency\n\n**Acceptance Criteria:**\n\nGiven I have HUGGINGFACEHUB_API_TOKEN set\nWhen I call get_chat_model(\"meta-llama/Llama-3.1-8B-Instruct\")\nThen HuggingFace model initializes via HuggingFaceEndpoint\n\nGiven I use HuggingFaceModelConfig\nWhen I set task, max_new_tokens, repetition_penalty\nThen parameters are passed correctly to HuggingFaceEndpoint\n\nGiven I call get_chat_model with HuggingFace model\nWhen initialization succeeds\nThen ChatHuggingFace instance is returned\n\nGiven HUGGINGFACEHUB_API_TOKEN is not set\nWhen I try to initialize HuggingFace model\nThen MissingAPIKeyError is raised with helpful message\n\nGiven I request an invalid HuggingFace model\nWhen initialization fails\nThen InvalidModelError is raised with model suggestions\n\nGiven I test tool calling support\nWhen I call supports_tool_calling() on HuggingFace model\nThen it returns True (HuggingFace supports tools)\n\n**Testing Considerations:**\n- Mock HuggingFaceEndpoint initialization\n- Test API key resolution from env\n- Test parameter passing to endpoint\n- Test error handling (no token, invalid model)\n- Integration test with real API (optional, marked slow)\n- Test streaming responses\n- Verify ChatHuggingFace wrapper works correctly\n- Test with multiple model names\n\n**Implementation Hints:**\n```python\n# In build_model_params()\nelif isinstance(model_config, HuggingFaceModelConfig):\n    if model_config.task is not None:\n        params[\"task\"] = model_config.task\n    if model_config.max_new_tokens is not None:\n        params[\"max_new_tokens\"] = model_config.max_new_tokens\n    if model_config.do_sample is not None:\n        params[\"do_sample\"] = model_config.do_sample\n    if model_config.repetition_penalty is not None:\n        params[\"repetition_penalty\"] = model_config.repetition_penalty\n    if model_config.top_p is not None:\n        params[\"top_p\"] = model_config.top_p\n    if model_config.top_k is not None:\n        params[\"top_k\"] = model_config.top_k\n    if model_config.model_kwargs is not None:\n        params.update(model_config.model_kwargs)\n\n# In get_chat_model(), after Ollama case\nelif provider == Provider.HUGGINGFACE:\n    from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n    \n    # Extract HuggingFace-specific params\n    hf_params = {\n        \"repo_id\": params.pop(\"model\"),\n        \"task\": params.pop(\"task\", \"text-generation\"),\n        \"max_new_tokens\": params.pop(\"max_new_tokens\", 512),\n        \"do_sample\": params.pop(\"do_sample\", True),\n    }\n    \n    # Add optional parameters\n    if \"repetition_penalty\" in params:\n        hf_params[\"repetition_penalty\"] = params.pop(\"repetition_penalty\")\n    if \"top_p\" in params:\n        hf_params[\"top_p\"] = params.pop(\"top_p\")\n    if \"top_k\" in params:\n        hf_params[\"top_k\"] = params.pop(\"top_k\")\n    \n    # Add API token\n    if resolved_api_key:\n        hf_params[\"huggingfacehub_api_token\"] = resolved_api_key\n    \n    # Initialize endpoint and wrap in ChatHuggingFace\n    try:\n        llm = HuggingFaceEndpoint(**hf_params)\n        return ChatHuggingFace(llm=llm, **params)\n    except Exception as e:\n        # Handle HuggingFace-specific errors\n        ...\n```\n\nFiles to modify:\n- src/consoul/ai/providers.py (add initialization logic)\n- pyproject.toml (add langchain-huggingface dependency)\n- tests/ai/test_providers.py (add tests)\n\nDependencies: SOUL-121, SOUL-122",
  "status": "todo",
  "type": "feature",
  "priority": "high",
  "labels": [
    "ai",
    "providers",
    "enhancement"
  ],
  "assignee": "jared@goatbytes.io",
  "reporter": "jared@goatbytes.io",
  "blocked_by": [],
  "blocks": [],
  "comments": [],
  "attachments": [],
  "attachment_count": 0,
  "comment_count": 0,
  "story_points": 8,
  "order": 0,
  "custom_fields": {}
}