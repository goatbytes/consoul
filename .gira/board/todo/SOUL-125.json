{
  "created_at": "2025-11-16T08:04:14.122743",
  "updated_at": "2025-11-16T08:04:14.122753",
  "id": "SOUL-125",
  "uuid": "263cb1e5-8f82-4483-9607-794c995a7cca",
  "title": "Add HuggingFace provider integration tests with real API",
  "description": "**As a** Consoul developer\n**I want** integration tests for HuggingFace provider\n**So that** we verify end-to-end functionality with real HuggingFace API\n\n**Context:**\nFollowing the testing pattern of other providers, create integration tests that verify:\n- Model initialization with API token\n- Basic chat completions\n- Streaming responses\n- Tool calling support\n- Error handling (missing token, invalid model, rate limits)\n- Multiple model sizes/types\n\nTests should be skippable when API token is not available (CI/local dev without credentials).\n\n**Technical Notes:**\n- tests/integration/test_huggingface.py - New test file\n- Use pytest markers: @pytest.mark.integration, @pytest.mark.slow\n- Skip if HUGGINGFACEHUB_API_TOKEN not set\n- Test with free/public models (avoid rate limits)\n- Mock responses for fast unit tests\n- Real API tests marked with @pytest.mark.slow\n- Test models: meta-llama/Llama-3.1-8B-Instruct, google/flan-t5-base\n- Verify supports_tool_calling() works correctly\n- Test both HuggingFaceEndpoint wrapper\n\n**Acceptance Criteria:**\n\nGiven I have HUGGINGFACEHUB_API_TOKEN configured\nWhen I run integration tests with --run-slow flag\nThen real HuggingFace API calls succeed\n\nGiven I run tests without API token\nWhen pytest executes integration suite\nThen HuggingFace tests are skipped gracefully\n\nGiven I test basic chat completion\nWhen I send a simple prompt\nThen response is generated successfully\n\nGiven I test streaming\nWhen I enable streaming mode\nThen tokens arrive progressively via iterator\n\nGiven I test with invalid model\nWhen I request a non-existent model\nThen appropriate error is raised with helpful message\n\nGiven I test tool calling\nWhen I bind tools to HuggingFace model\nThen supports_tool_calling() returns True\n\n**Testing Considerations:**\n- Use small/fast models for testing (avoid heavy models)\n- Mock HuggingFaceEndpoint for fast tests\n- Real API tests are optional (@pytest.mark.slow)\n- Test timeout handling\n- Test rate limit errors gracefully\n- Verify API token masking in logs\n- Test multiple model formats (org/model-name)\n\n**Implementation Hints:**\n```python\nimport pytest\nimport os\nfrom consoul.ai import get_chat_model\nfrom consoul.config.models import HuggingFaceModelConfig, Provider\n\n@pytest.fixture\ndef skip_if_no_hf_token():\n    if not os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"):\n        pytest.skip(\"HuggingFace API token not available\")\n\n# Fast test with mocks\n@pytest.mark.integration\ndef test_huggingface_initialization_mock():\n    \"\"\"Test HuggingFace model init with mocked endpoint.\"\"\"\n    # Mock HuggingFaceEndpoint to avoid API call\n    ...\n\n# Real API test\n@pytest.mark.slow\n@pytest.mark.integration\ndef test_huggingface_chat_real_api(skip_if_no_hf_token):\n    \"\"\"Test real HuggingFace API call.\"\"\"\n    config = HuggingFaceModelConfig(\n        model=\"google/flan-t5-base\",\n        temperature=0.7,\n        max_new_tokens=100\n    )\n    model = get_chat_model(config)\n    \n    response = model.invoke(\"What is 2+2?\")\n    assert response.content\n    assert len(response.content) > 0\n\n@pytest.mark.slow\n@pytest.mark.integration\ndef test_huggingface_streaming(skip_if_no_hf_token):\n    \"\"\"Test streaming responses.\"\"\"\n    model = get_chat_model(\"meta-llama/Llama-3.1-8B-Instruct\")\n    \n    chunks = []\n    for chunk in model.stream(\"Tell me a joke\"):\n        chunks.append(chunk)\n    \n    assert len(chunks) > 0\n    assert all(hasattr(c, \"content\") for c in chunks)\n\n@pytest.mark.integration\ndef test_huggingface_tool_calling_support():\n    \"\"\"Test tool calling capability detection.\"\"\"\n    from consoul.ai import supports_tool_calling\n    \n    model = get_chat_model(\"meta-llama/Llama-3.1-8B-Instruct\")\n    assert supports_tool_calling(model) is True\n\n@pytest.mark.integration\ndef test_huggingface_missing_token_error():\n    \"\"\"Test error when API token is missing.\"\"\"\n    # Temporarily unset token\n    token = os.environ.pop(\"HUGGINGFACEHUB_API_TOKEN\", None)\n    \n    try:\n        with pytest.raises(MissingAPIKeyError, match=\"HUGGINGFACEHUB_API_TOKEN\"):\n            get_chat_model(\"meta-llama/Llama-3.1-8B-Instruct\")\n    finally:\n        if token:\n            os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = token\n```\n\nFiles to create:\n- tests/integration/test_huggingface.py\n- tests/fixtures/hf_responses.json (mock responses)\n\nDependencies: SOUL-121, SOUL-122, SOUL-123, SOUL-124",
  "status": "todo",
  "type": "task",
  "priority": "medium",
  "labels": [
    "testing",
    "ai",
    "providers"
  ],
  "reporter": "jared@goatbytes.io",
  "blocked_by": [],
  "blocks": [],
  "comments": [],
  "attachments": [],
  "attachment_count": 0,
  "comment_count": 0,
  "story_points": 3,
  "order": 0,
  "custom_fields": {}
}