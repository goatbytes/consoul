{
  "created_at": "2025-12-23T12:09:53.180172",
  "updated_at": "2025-12-23T23:09:24.690824Z",
  "id": "SOUL-295",
  "uuid": "cade3e22-667b-42f4-a616-e24388d4b8b7",
  "title": "Add structured logging and compliance audit trail",
  "description": "**As a** legal tech operator\n**I want** structured JSON logging with audit trail for tool execution\n**So that** I can demonstrate compliance and debug production issues\n\n**Context:**\nLegal industry requires audit trails for compliance (who accessed what documents, what AI actions were taken). Current logging uses standard Python logger without structured output. Need JSON-formatted logs with:\n- Request/response tracking\n- Tool execution audit\n- User/session identification\n- Timestamp precision\n\nLangChain production best practices emphasize structured logging for debugging multi-step chains.\n\n**Technical Notes:**\n- Create: src/consoul/sdk/logging_config.py\n- Use: python-json-logger or structlog\n- Add: AuditLogger class for compliance events\n- Integration: ConversationService, ToolService\n- Format: JSON with correlation IDs\n- Destination: stdout (captured by log aggregation)\n\nLog types needed:\n1. Request logs: session_id, user_id, message\n2. Response logs: tokens, model, latency\n3. Tool execution logs: tool_name, args, result, approver\n4. Error logs: exception, stack trace, context\n\n**Acceptance Criteria:**\n\n**Given** SDK configured with structured logging\n**When** message is sent\n**Then** logs JSON: {\"event\": \"message\", \"session_id\": \"...\", \"timestamp\": \"...\"}\n\n**Given** tool is executed\n**When** using audit mode\n**Then** logs: {\"event\": \"tool_execution\", \"tool\": \"bash\", \"user\": \"...\", \"args\": {...}, \"approved_by\": \"...\"}\n\n**Given** error occurs\n**When** exception is raised\n**Then** logs: {\"event\": \"error\", \"exception\": \"...\", \"traceback\": \"...\", \"context\": {...}}\n\n**Given** production deployment\n**When** reviewing logs\n**Then** can filter by session_id, user_id, or event type\n\n**Testing Considerations:**\n- Test log output format (valid JSON)\n- Test log levels (INFO, WARNING, ERROR)\n- Test correlation ID propagation\n- Test PII redaction (if needed)\n- Verify log aggregation compatibility (Datadog, Splunk)\n- Test audit query patterns\n\n**Implementation Hints:**\n- Use structlog for structured logging\n- Add correlation_id to all logs (from request context)\n- Create AuditLogger.log_tool_execution(event)\n- Redact sensitive data (API keys, passwords)\n- Add log_level config option\n- JSON format: {\"timestamp\": ISO8601, \"level\": \"INFO\", \"event\": \"...\", ...}\n- Document compliance queries in docs/operations/audit.md\n- Add example: examples/logging/compliance_audit.py\n- Integration: Pass logger to ConversationService\n- Reference: SOUL-292 for tool execution hooks",
  "status": "todo",
  "type": "story",
  "priority": "high",
  "labels": [
    "sdk",
    "observability",
    "compliance",
    "logging"
  ],
  "assignee": "jared@goatbytes.io",
  "reporter": "jared@goatbytes.io",
  "epic_id": "EPIC-016",
  "blocked_by": [],
  "blocks": [],
  "comments": [],
  "attachments": [],
  "attachment_count": 0,
  "comment_count": 0,
  "story_points": 3,
  "order": 0,
  "custom_fields": {}
}