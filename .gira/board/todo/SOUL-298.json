{
  "created_at": "2025-12-23T12:11:44.998379",
  "updated_at": "2025-12-23T23:09:24.694671Z",
  "id": "SOUL-298",
  "uuid": "81038db9-2392-43f9-bf8b-fd0c49c970d8",
  "title": "Add WebSocket streaming endpoint with backpressure",
  "description": "**As a** frontend developer\n**I want** WebSocket /ws/chat endpoint with token streaming\n**So that** users see real-time AI responses with typing indicators\n\n**Context:**\nLegal industry UI needs progressive response display (better UX than waiting for full response). WebSocket enables:\n- Token-by-token streaming\n- Tool approval workflow (bidirectional)\n- Connection state management\n- Reconnection support\n\nCurrent example (examples/fastapi_websocket_server.py) shows pattern - needs production hardening.\n\n**Technical Notes:**\n- Create: src/consoul/server/endpoints/websocket.py\n- Integration: create_server() mounts WS /ws/chat/{session_id}\n- Use: ConversationService.send_message() async streaming\n- Protocol: JSON events over WebSocket\n- Dependencies: fastapi WebSocket support\n\nEvent protocol:\n- Client → Server: {\"type\": \"message\", \"content\": \"...\"}\n- Server → Client: {\"type\": \"token\", \"data\": {\"text\": \"...\"}}\n- Server → Client: {\"type\": \"tool_approval_request\", \"data\": {...}}\n- Server → Client: {\"type\": \"done\", \"data\": {\"usage\": {...}}}\n- Server → Client: {\"type\": \"error\", \"data\": {\"message\": \"...\"}}\n\nBackpressure handling:\n- Detect slow clients (send queue full)\n- Buffer tokens (max 1000)\n- Drop connection if client can't keep up\n\n**Acceptance Criteria:**\n\n**Given** WebSocket connection established\n**When** client sends {\"type\": \"message\", \"content\": \"Hello\"}\n**Then** receives stream of {\"type\": \"token\"} events until done\n\n**Given** AI makes tool call\n**When** tool requires approval\n**Then** sends {\"type\": \"tool_approval_request\"} and waits for client response\n\n**Given** client connection is slow\n**When** send buffer exceeds limit\n**Then** disconnects with {\"type\": \"error\", \"data\": {\"message\": \"client too slow\"}}\n\n**Given** WebSocket disconnects\n**When** client reconnects with same session_id\n**Then** conversation history is preserved\n\n**Given** error occurs during streaming\n**When** exception is raised\n**Then** sends {\"type\": \"error\"} and closes connection gracefully\n\n**Testing Considerations:**\n- Test WebSocket connection lifecycle\n- Test token streaming flow\n- Test tool approval workflow\n- Test backpressure detection\n- Test reconnection handling\n- Test concurrent connections per session\n- Use FastAPI WebSocketTestSession for testing\n\n**Implementation Hints:**\n- WebSocket endpoint: @app.websocket(\"/ws/chat/{session_id}\")\n- Accept initial message after connection\n- Stream tokens: async for token in service.send_message(...)\n- Send events: await websocket.send_json(event)\n- Backpressure: Monitor websocket._send_queue size\n- Graceful close: await websocket.close(code=1000)\n- Error handling: Try/except with error event\n- Tool approval: Bidirectional event flow\n- Session management: Same SessionStore as HTTP\n- Add connection count to /health\n- Document protocol in docs/server/websocket-protocol.md\n- Example client: examples/server/websocket_client.html\n- Reference: SOUL-297 for SessionStore integration\n- Reference: ConversationService async streaming",
  "status": "todo",
  "type": "story",
  "priority": "high",
  "labels": [
    "server",
    "websocket",
    "streaming",
    "production"
  ],
  "assignee": "jared@goatbytes.io",
  "reporter": "jared@goatbytes.io",
  "epic_id": "EPIC-017",
  "blocked_by": [
    "SOUL-297"
  ],
  "blocks": [],
  "comments": [],
  "attachments": [],
  "attachment_count": 0,
  "comment_count": 0,
  "story_points": 8,
  "order": 0,
  "custom_fields": {}
}