2025-12-01 00:22:30,336 - consoul.tui.cli - INFO - Debug logging enabled, writing to: ./consoul-log.txt
2025-12-01 00:22:30,809 - consoul.tui.app - INFO - [PERF] Step 1 (Load config): 104.7ms
2025-12-01 00:22:31,997 - consoul.tui.app - INFO - [PERF] Step 2 (Initialize AI model): 1187.7ms
2025-12-01 00:22:31,998 - consoul.tui.app - INFO - [PERF-CONV] Get conversation config: 0.0ms
2025-12-01 00:22:31,999 - consoul.ai.history - DEBUG - [PERF-HIST] Get model token limit: 0.3ms
2025-12-01 00:22:32,082 - consoul.ai.history - DEBUG - [PERF-HIST] Create token counter: 83.1ms
2025-12-01 00:22:32,091 - consoul.ai.history - DEBUG - [PERF-HIST] Create ConversationDatabase: 2.3ms
2025-12-01 00:22:32,091 - consoul.ai.history - DEBUG - [PERF-HIST] Total ConversationHistory.__init__: 92.5ms
2025-12-01 00:22:32,091 - consoul.tui.app - INFO - [PERF-CONV] ConversationHistory.__init__: 92.6ms
2025-12-01 00:22:32,091 - consoul.tui.app - INFO - [PERF] Step 3 (Create conversation): 94.0ms
2025-12-01 00:22:32,091 - consoul.tui.app - INFO - Initialized AI model: gpt-oss:20b, session: None
2025-12-01 00:22:32,095 - consoul.tui.app - INFO - [PERF] Step 4 (Load tools): 4.1ms
2025-12-01 00:22:32,115 - consoul.tui.app - INFO - [PERF] Step 5 (Bind tools): 20.3ms
2025-12-01 00:22:32,132 - consoul.tui.app - INFO - [PERF] Cleanup old conversations: 16.2ms
2025-12-01 00:22:32,169 - consoul.tui.app - INFO - [PERF] Initialize title generator: 37.6ms
2025-12-01 00:22:32,170 - consoul.tui.app - INFO - [PERF] Applied theme: consoul-light
2025-12-01 00:22:32,466 - consoul.tui.app - INFO - [PERF] Apply theme: 296.6ms
2025-12-01 00:22:32,972 - consoul.tui.app - INFO - [POST-INIT] Post-initialization setup complete
2025-12-01 00:22:34,539 - consoul.tui.widgets.chat_view - DEBUG - [SCROLL] Adding message - role: user, auto_scroll: True, current_scroll_y: 0.0, max_scroll_y: 0
2025-12-01 00:22:34,570 - consoul.tui.widgets.chat_view - INFO - [SCROLL] Scheduling scroll_end after message add - role: user, scroll_y: 0.0
2025-12-01 00:22:34,576 - consoul.tui.widgets.chat_view - DEBUG - [SCROLL] Showing typing indicator
2025-12-01 00:22:34,605 - consoul.tui.app - DEBUG - [MESSAGE_SUBMIT] is_first_message=True, persist=True, _conversation_created=False, session_id=None, message_count=0
2025-12-01 00:22:34,605 - consoul.tui.app - INFO - [IMAGE_DETECTION] Auto-detect enabled: True, Attached images: 0, Auto-detected: 0, Total (deduplicated): 0
2025-12-01 00:22:34,605 - consoul.tui.app - INFO - [IMAGE_DETECTION] Checking vision support for model: gpt-oss:20b
2025-12-01 00:22:34,605 - consoul.tui.app - INFO - [IMAGE_DETECTION] Model 'gpt-oss:20b' has vision: False
2025-12-01 00:22:34,605 - consoul.tui.app - INFO - [IMAGE_DETECTION] Model supports vision: False
2025-12-01 00:22:34,605 - consoul.tui.app - INFO - [IMAGE_DETECTION] Condition check: image_paths=False, model_supports_vision=False, combined=False
2025-12-01 00:22:34,608 - consoul.tui.widgets.chat_view - DEBUG - [SCROLL] Scrolling after typing indicator layout - scroll_y: 0.0, max_scroll_y: 0
2025-12-01 00:22:34,610 - consoul.tui.app - INFO - Created conversation session: ff57e7af-c4bf-448e-abb2-c4cfa5c9c9d6
2025-12-01 00:22:34,611 - consoul.tui.app - DEBUG - Persisted user message with ID: 1038
2025-12-01 00:22:34,663 - consoul.tui.app - DEBUG - [TOOL_FLOW] _stream_ai_response ENTRY - iteration 0/5
2025-12-01 00:22:34,663 - consoul.tui.app - DEBUG - [MESSAGES] Before trimming: conversation has 1 messages
2025-12-01 00:22:34,663 - consoul.tui.app - DEBUG - [MESSAGES]   Message 0: type=human, content_len=146
2025-12-01 00:22:34,664 - consoul.ai.history - DEBUG - [TRIM_DEBUG] After trimming: 1 messages, original had: 1 messages
2025-12-01 00:22:34,664 - consoul.ai.history - DEBUG - [TRIM_DEBUG]   Trimmed msg 0: type=human, isinstance(SystemMessage)=False
2025-12-01 00:22:34,664 - consoul.ai.history - DEBUG - [TRIM_DEBUG] Non-system messages in trimmed result: 1, total original messages: 1
2025-12-01 00:22:34,664 - consoul.tui.app - DEBUG - [MESSAGES] Sending 1 messages to model
2025-12-01 00:22:34,664 - consoul.tui.app - INFO - [IMAGE_DETECTION] Last message has multimodal content: False
2025-12-01 00:22:34,665 - consoul.tui.app - DEBUG - [MESSAGES] Converted 1 messages to 1 dict messages
2025-12-01 00:22:45,734 - consoul.tui.app - DEBUG - [TOOL_FLOW] Stream loop finished. Collected 491 chunks, exception=None
2025-12-01 00:22:45,734 - consoul.tui.app - DEBUG - Found 1 tool_calls after merging chunks
2025-12-01 00:22:45,735 - consoul.tui.app - DEBUG - [TOOL_FLOW] Got first_token: is_none=True, value=None, ttft=11.070s
2025-12-01 00:22:45,735 - consoul.tui.app - DEBUG - Final tool_calls: [{'name': 'bash_execute', 'args': {'command': 'ls -1', 'timeout': None, 'working_directory': None}, 'id': 'a01a8a21-b14e-4353-9d8b-6aac583c08a1', 'type': 'tool_call'}]
2025-12-01 00:22:45,735 - consoul.tui.app - DEBUG - [COST] Checking 491 chunks for usage data
2025-12-01 00:22:45,735 - consoul.tui.app - DEBUG - [COST] Found usage_metadata in chunk 489: {'input_tokens': 5068, 'output_tokens': 525, 'total_tokens': 5593}
2025-12-01 00:22:45,735 - consoul.tui.app - DEBUG - Final message has 1 tool_calls
2025-12-01 00:22:45,736 - consoul.tui.app - DEBUG - [TOOL_FLOW] About to send final_message to main thread
2025-12-01 00:22:45,736 - consoul.tui.app - DEBUG - [TOOL_FLOW] Sending final_message to queue: has_message=True
2025-12-01 00:22:45,737 - consoul.tui.widgets.chat_view - DEBUG - [SCROLL] Adding message - role: unknown, auto_scroll: True, current_scroll_y: 0.0, max_scroll_y: 0
2025-12-01 00:22:45,739 - consoul.tui.widgets.chat_view - INFO - [SCROLL] Scheduling scroll_end after message add - role: unknown, scroll_y: 0.0
2025-12-01 00:22:45,740 - consoul.tui.widgets.streaming_response - INFO - [SCROLL] Finalizing stream - token_count: 0, height_before: 0, parent_scroll_y: 0.0
2025-12-01 00:22:45,740 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Stopping auto-scroll timer
2025-12-01 00:22:45,740 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Scrolling self to bottom - height_after: 0, scroll_y: 0.0, max_scroll_y: 0
2025-12-01 00:22:45,740 - consoul.tui.widgets.streaming_response - INFO - [SCROLL] Requesting parent scroll_end - parent_scroll_y: 0.0, parent_max_scroll_y: 0, parent_height: Size(width=170, height=44)
2025-12-01 00:22:45,740 - consoul.tui.app - DEBUG - [TOOL_FLOW] Stream exception: None
2025-12-01 00:22:45,740 - consoul.tui.app - DEBUG - [TOOL_FLOW] Got final_message from queue: type=AIMessage, is_none=False
2025-12-01 00:22:45,740 - consoul.tui.app - DEBUG - [TOOL_FLOW] Response check: has_content=, has_tool_calls_in_message=[{'name': 'bash_execute', 'args': {'command': 'ls -1', 'timeout': None, 'working_directory': None}, 'id': 'a01a8a21-b14e-4353-9d8b-6aac583c08a1', 'type': 'tool_call'}], full_response_len=0, cancelled=False
2025-12-01 00:22:45,749 - consoul.tui.app - DEBUG - [TOOL_FLOW] Checking final_message for tool calls: has_final_message=True
2025-12-01 00:22:45,749 - consoul.tui.app - DEBUG - [TOOL_FLOW] Calling has_tool_calls()
2025-12-01 00:22:45,749 - consoul.tui.app - DEBUG - [TOOL_FLOW] Tool calls detected in model response: 1 call(s), content_length=0
2025-12-01 00:22:45,749 - consoul.tui.app - DEBUG - [TOOL_FLOW] Replacing empty stream widget with tool execution message
2025-12-01 00:22:45,750 - consoul.tui.widgets.chat_view - DEBUG - [SCROLL] Adding message - role: system, auto_scroll: True, current_scroll_y: 0.0, max_scroll_y: 0
2025-12-01 00:22:45,752 - consoul.tui.widgets.chat_view - INFO - [SCROLL] Scheduling scroll_end after message add - role: system, scroll_y: 0.0
2025-12-01 00:22:45,752 - consoul.tui.app - DEBUG - [TOOL_FLOW] Calling _handle_tool_calls with 1 calls
2025-12-01 00:22:45,752 - consoul.tui.app - DEBUG - [TOOL_FLOW] Tool call iteration 1 with 1 tool(s)
2025-12-01 00:22:45,752 - consoul.tui.app - DEBUG - [TOOL_FLOW] Reset tool results for new batch of 1 tools
2025-12-01 00:22:45,752 - consoul.tui.app - DEBUG - [TOOL_FLOW] Tool call detected: bash_execute with args: {'command': 'ls -1', 'timeout': None, 'working_directory': None}
2025-12-01 00:22:45,752 - consoul.tui.app - DEBUG - [TOOL_FLOW] Posting ToolApprovalRequested for bash_execute
2025-12-01 00:22:45,752 - consoul.tui.app - DEBUG - [TOOL_FLOW] Posted ToolApprovalRequested for bash_execute
2025-12-01 00:22:45,752 - consoul.tui.app - DEBUG - [TOOL_FLOW] _handle_tool_calls completed
2025-12-01 00:22:45,754 - consoul.tui.app - DEBUG - [SCROLL] Before removing StreamingResponse - scroll_y: 0.0, max_scroll_y: 0, at_bottom: True
2025-12-01 00:22:45,756 - consoul.tui.app - DEBUG - [TOOL_FLOW] on_tool_approval_requested called for bash_execute
2025-12-01 00:22:45,759 - asyncio - ERROR - _GatheringFuture exception was never retrieved
future: <_GatheringFuture finished exception=TypeError("Input data should be a string, not <class 'rich.text.Text'>")>
Traceback (most recent call last):
  File "/Users/jaredrummler/.pyenv/versions/3.12.3/lib/python3.12/site-packages/textual/widgets/_markdown.py", line 1340, in await_update
    tokens = await asyncio.get_running_loop().run_in_executor(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredrummler/.pyenv/versions/3.12.3/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredrummler/.pyenv/versions/3.12.3/lib/python3.12/site-packages/markdown_it/main.py", line 274, in parse
    raise TypeError(f"Input data should be a string, not {type(src)}")
TypeError: Input data should be a string, not <class 'rich.text.Text'>
2025-12-01 00:22:45,761 - consoul.tui.app - DEBUG - [TOOL_FLOW] Checking if approval needed for bash_execute
2025-12-01 00:22:45,761 - consoul.tui.app - DEBUG - [TOOL_FLOW] Approval check result: needs_approval=False
2025-12-01 00:22:45,761 - consoul.tui.app - DEBUG - [TOOL_FLOW] Auto-approving bash_execute
2025-12-01 00:22:45,762 - asyncio - ERROR - _GatheringFuture exception was never retrieved
future: <_GatheringFuture finished exception=TypeError("Input data should be a string, not <class 'rich.text.Text'>")>
Traceback (most recent call last):
  File "/Users/jaredrummler/.pyenv/versions/3.12.3/lib/python3.12/site-packages/textual/widgets/_markdown.py", line 1340, in await_update
    tokens = await asyncio.get_running_loop().run_in_executor(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredrummler/.pyenv/versions/3.12.3/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredrummler/.pyenv/versions/3.12.3/lib/python3.12/site-packages/markdown_it/main.py", line 274, in parse
    raise TypeError(f"Input data should be a string, not {type(src)}")
TypeError: Input data should be a string, not <class 'rich.text.Text'>
2025-12-01 00:22:45,762 - asyncio - ERROR - _GatheringFuture exception was never retrieved
future: <_GatheringFuture finished exception=TypeError("Input data should be a string, not <class 'rich.text.Text'>")>
Traceback (most recent call last):
  File "/Users/jaredrummler/.pyenv/versions/3.12.3/lib/python3.12/site-packages/textual/widgets/_markdown.py", line 1340, in await_update
    tokens = await asyncio.get_running_loop().run_in_executor(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredrummler/.pyenv/versions/3.12.3/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredrummler/.pyenv/versions/3.12.3/lib/python3.12/site-packages/markdown_it/main.py", line 274, in parse
    raise TypeError(f"Input data should be a string, not {type(src)}")
TypeError: Input data should be a string, not <class 'rich.text.Text'>
2025-12-01 00:22:45,766 - consoul.tui.app - DEBUG - [TOOL_FLOW] on_tool_approval_result: tool=bash_execute, approved=True, call_id=a01a8a21-b14e-4353-9d8b-6aac583c08a1
2025-12-01 00:22:45,766 - consoul.tui.app - INFO - [TOOL_FLOW] Executing approved tool: bash_execute
2025-12-01 00:22:45,806 - consoul.tui.app - DEBUG - [TOOL_FLOW] Tool completion status: 1/1 tools completed
2025-12-01 00:22:45,806 - consoul.tui.app - DEBUG - [TOOL_FLOW] All 1 tools completed, posting message to continue
2025-12-01 00:22:45,807 - consoul.tui.app - DEBUG - [TOOL_FLOW] _stream_ai_response ENTRY - iteration 1/5
2025-12-01 00:22:45,808 - consoul.tui.app - DEBUG - [MESSAGES] Before trimming: conversation has 3 messages
2025-12-01 00:22:45,808 - consoul.tui.app - DEBUG - [MESSAGES]   Message 0: type=human, content_len=146
2025-12-01 00:22:45,808 - consoul.tui.app - DEBUG - [MESSAGES]   Message 1: type=ai, content_len=0
2025-12-01 00:22:45,808 - consoul.tui.app - DEBUG - [MESSAGES]   Message 2: type=tool, content_len=375
2025-12-01 00:22:45,808 - consoul.ai.history - DEBUG - [TRIM_DEBUG] After trimming: 3 messages, original had: 3 messages
2025-12-01 00:22:45,809 - consoul.ai.history - DEBUG - [TRIM_DEBUG]   Trimmed msg 0: type=human, isinstance(SystemMessage)=False
2025-12-01 00:22:45,809 - consoul.ai.history - DEBUG - [TRIM_DEBUG]   Trimmed msg 1: type=ai, isinstance(SystemMessage)=False
2025-12-01 00:22:45,809 - consoul.ai.history - DEBUG - [TRIM_DEBUG]   Trimmed msg 2: type=tool, isinstance(SystemMessage)=False
2025-12-01 00:22:45,809 - consoul.ai.history - DEBUG - [TRIM_DEBUG] Non-system messages in trimmed result: 3, total original messages: 3
2025-12-01 00:22:45,809 - consoul.tui.app - DEBUG - [MESSAGES] Sending 3 messages to model
2025-12-01 00:22:45,809 - consoul.tui.app - INFO - [IMAGE_DETECTION] Last message has multimodal content: False
2025-12-01 00:22:45,809 - consoul.tui.app - DEBUG - [MESSAGES] Converted 3 messages to 3 dict messages
2025-12-01 00:22:47,558 - consoul.tui.app - DEBUG - [TOOL_FLOW] Stream loop finished. Collected 21 chunks, exception=None
2025-12-01 00:22:47,558 - consoul.tui.app - DEBUG - Found 1 tool_calls after merging chunks
2025-12-01 00:22:47,558 - consoul.tui.app - DEBUG - Final tool_calls: [{'name': 'read_file', 'args': {'end_page': None, 'file_path': 'README.md', 'limit': None, 'offset': None, 'start_page': None}, 'id': 'b7c8637d-343c-4225-acd1-856cbadc1db9', 'type': 'tool_call'}]
2025-12-01 00:22:47,558 - consoul.tui.app - DEBUG - [TOOL_FLOW] Got first_token: is_none=True, value=None, ttft=1.749s
2025-12-01 00:22:47,558 - consoul.tui.app - DEBUG - [COST] Checking 21 chunks for usage data
2025-12-01 00:22:47,558 - consoul.tui.widgets.chat_view - DEBUG - [SCROLL] Adding message - role: unknown, auto_scroll: True, current_scroll_y: 0.0, max_scroll_y: 0
2025-12-01 00:22:47,559 - consoul.tui.app - DEBUG - [COST] Found usage_metadata in chunk 19: {'input_tokens': 5229, 'output_tokens': 63, 'total_tokens': 5292}
2025-12-01 00:22:47,563 - consoul.tui.app - DEBUG - Final message has 1 tool_calls
2025-12-01 00:22:47,564 - consoul.tui.widgets.chat_view - INFO - [SCROLL] Scheduling scroll_end after message add - role: unknown, scroll_y: 0.0
2025-12-01 00:22:47,564 - consoul.tui.app - DEBUG - [TOOL_FLOW] About to send final_message to main thread
2025-12-01 00:22:47,564 - consoul.tui.widgets.streaming_response - INFO - [SCROLL] Finalizing stream - token_count: 0, height_before: 0, parent_scroll_y: 0.0
2025-12-01 00:22:47,564 - consoul.tui.app - DEBUG - [TOOL_FLOW] Sending final_message to queue: has_message=True
2025-12-01 00:22:47,564 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Stopping auto-scroll timer
2025-12-01 00:22:47,564 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Scrolling self to bottom - height_after: 0, scroll_y: 0.0, max_scroll_y: 0
2025-12-01 00:22:47,564 - consoul.tui.widgets.streaming_response - INFO - [SCROLL] Requesting parent scroll_end - parent_scroll_y: 0.0, parent_max_scroll_y: 0, parent_height: Size(width=170, height=44)
2025-12-01 00:22:47,564 - consoul.tui.app - DEBUG - [TOOL_FLOW] Stream exception: None
2025-12-01 00:22:47,567 - consoul.tui.app - DEBUG - [TOOL_FLOW] Got final_message from queue: type=AIMessage, is_none=False
2025-12-01 00:22:47,567 - consoul.tui.app - DEBUG - [TOOL_FLOW] Response check: has_content=, has_tool_calls_in_message=[{'name': 'read_file', 'args': {'end_page': None, 'file_path': 'README.md', 'limit': None, 'offset': None, 'start_page': None}, 'id': 'b7c8637d-343c-4225-acd1-856cbadc1db9', 'type': 'tool_call'}], full_response_len=0, cancelled=False
2025-12-01 00:22:47,569 - consoul.tui.app - DEBUG - [TOOL_FLOW] Checking final_message for tool calls: has_final_message=True
2025-12-01 00:22:47,569 - consoul.tui.app - DEBUG - [TOOL_FLOW] Calling has_tool_calls()
2025-12-01 00:22:47,569 - consoul.tui.app - DEBUG - [TOOL_FLOW] Tool calls detected in model response: 1 call(s), content_length=0
2025-12-01 00:22:47,569 - consoul.tui.app - DEBUG - [TOOL_FLOW] Replacing empty stream widget with tool execution message
2025-12-01 00:22:47,570 - consoul.tui.widgets.chat_view - DEBUG - [SCROLL] Adding message - role: system, auto_scroll: True, current_scroll_y: 0.0, max_scroll_y: 0
2025-12-01 00:22:47,573 - consoul.tui.widgets.chat_view - INFO - [SCROLL] Scheduling scroll_end after message add - role: system, scroll_y: 0.0
2025-12-01 00:22:47,573 - consoul.tui.app - DEBUG - [TOOL_FLOW] Calling _handle_tool_calls with 1 calls
2025-12-01 00:22:47,573 - consoul.tui.app - DEBUG - [TOOL_FLOW] Tool call iteration 2 with 1 tool(s)
2025-12-01 00:22:47,573 - consoul.tui.app - DEBUG - [TOOL_FLOW] Reset tool results for new batch of 1 tools
2025-12-01 00:22:47,573 - consoul.tui.app - DEBUG - [TOOL_FLOW] Tool call detected: read_file with args: {'end_page': None, 'file_path': 'README.md', 'limit': None, 'offset': None, 'start_page': None}
2025-12-01 00:22:47,573 - consoul.tui.app - DEBUG - [TOOL_FLOW] Posting ToolApprovalRequested for read_file
2025-12-01 00:22:47,573 - consoul.tui.app - DEBUG - [TOOL_FLOW] Posted ToolApprovalRequested for read_file
2025-12-01 00:22:47,573 - consoul.tui.app - DEBUG - [TOOL_FLOW] _handle_tool_calls completed
2025-12-01 00:22:47,573 - consoul.tui.app - DEBUG - [SCROLL] Before removing StreamingResponse - scroll_y: 0.0, max_scroll_y: 0, at_bottom: True
2025-12-01 00:22:47,575 - consoul.tui.app - DEBUG - [TOOL_FLOW] on_tool_approval_requested called for read_file
2025-12-01 00:22:47,576 - asyncio - ERROR - _GatheringFuture exception was never retrieved
future: <_GatheringFuture finished exception=TypeError("Input data should be a string, not <class 'rich.text.Text'>")>
Traceback (most recent call last):
  File "/Users/jaredrummler/.pyenv/versions/3.12.3/lib/python3.12/site-packages/textual/widgets/_markdown.py", line 1340, in await_update
    tokens = await asyncio.get_running_loop().run_in_executor(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredrummler/.pyenv/versions/3.12.3/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredrummler/.pyenv/versions/3.12.3/lib/python3.12/site-packages/markdown_it/main.py", line 274, in parse
    raise TypeError(f"Input data should be a string, not {type(src)}")
TypeError: Input data should be a string, not <class 'rich.text.Text'>
2025-12-01 00:22:47,577 - asyncio - ERROR - _GatheringFuture exception was never retrieved
future: <_GatheringFuture finished exception=TypeError("Input data should be a string, not <class 'rich.text.Text'>")>
Traceback (most recent call last):
  File "/Users/jaredrummler/.pyenv/versions/3.12.3/lib/python3.12/site-packages/textual/widgets/_markdown.py", line 1340, in await_update
    tokens = await asyncio.get_running_loop().run_in_executor(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredrummler/.pyenv/versions/3.12.3/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredrummler/.pyenv/versions/3.12.3/lib/python3.12/site-packages/markdown_it/main.py", line 274, in parse
    raise TypeError(f"Input data should be a string, not {type(src)}")
TypeError: Input data should be a string, not <class 'rich.text.Text'>
2025-12-01 00:22:47,577 - consoul.tui.app - DEBUG - [TOOL_FLOW] Checking if approval needed for read_file
2025-12-01 00:22:47,578 - consoul.tui.app - DEBUG - [TOOL_FLOW] Approval check result: needs_approval=False
2025-12-01 00:22:47,578 - consoul.tui.app - DEBUG - [TOOL_FLOW] Auto-approving read_file
2025-12-01 00:22:47,578 - asyncio - ERROR - _GatheringFuture exception was never retrieved
future: <_GatheringFuture finished exception=TypeError("Input data should be a string, not <class 'rich.text.Text'>")>
Traceback (most recent call last):
  File "/Users/jaredrummler/.pyenv/versions/3.12.3/lib/python3.12/site-packages/textual/widgets/_markdown.py", line 1340, in await_update
    tokens = await asyncio.get_running_loop().run_in_executor(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredrummler/.pyenv/versions/3.12.3/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredrummler/.pyenv/versions/3.12.3/lib/python3.12/site-packages/markdown_it/main.py", line 274, in parse
    raise TypeError(f"Input data should be a string, not {type(src)}")
TypeError: Input data should be a string, not <class 'rich.text.Text'>
2025-12-01 00:22:47,582 - consoul.tui.app - DEBUG - [TOOL_FLOW] on_tool_approval_result: tool=read_file, approved=True, call_id=b7c8637d-343c-4225-acd1-856cbadc1db9
2025-12-01 00:22:47,582 - consoul.tui.app - INFO - [TOOL_FLOW] Executing approved tool: read_file
2025-12-01 00:22:47,597 - consoul.tui.app - DEBUG - [TOOL_FLOW] Tool completion status: 1/1 tools completed
2025-12-01 00:22:47,597 - consoul.tui.app - DEBUG - [TOOL_FLOW] All 1 tools completed, posting message to continue
2025-12-01 00:22:47,598 - consoul.tui.app - DEBUG - [TOOL_FLOW] _stream_ai_response ENTRY - iteration 2/5
2025-12-01 00:22:47,598 - consoul.tui.app - DEBUG - [MESSAGES] Before trimming: conversation has 5 messages
2025-12-01 00:22:47,598 - consoul.tui.app - DEBUG - [MESSAGES]   Message 0: type=human, content_len=146
2025-12-01 00:22:47,598 - consoul.tui.app - DEBUG - [MESSAGES]   Message 1: type=ai, content_len=0
2025-12-01 00:22:47,598 - consoul.tui.app - DEBUG - [MESSAGES]   Message 2: type=tool, content_len=375
2025-12-01 00:22:47,598 - consoul.tui.app - DEBUG - [MESSAGES]   Message 3: type=ai, content_len=0
2025-12-01 00:22:47,598 - consoul.tui.app - DEBUG - [MESSAGES]   Message 4: type=tool, content_len=7048
2025-12-01 00:22:47,599 - consoul.ai.history - DEBUG - [TRIM_DEBUG] After trimming: 5 messages, original had: 5 messages
2025-12-01 00:22:47,599 - consoul.ai.history - DEBUG - [TRIM_DEBUG]   Trimmed msg 0: type=human, isinstance(SystemMessage)=False
2025-12-01 00:22:47,599 - consoul.ai.history - DEBUG - [TRIM_DEBUG]   Trimmed msg 1: type=ai, isinstance(SystemMessage)=False
2025-12-01 00:22:47,599 - consoul.ai.history - DEBUG - [TRIM_DEBUG]   Trimmed msg 2: type=tool, isinstance(SystemMessage)=False
2025-12-01 00:22:47,599 - consoul.ai.history - DEBUG - [TRIM_DEBUG]   Trimmed msg 3: type=ai, isinstance(SystemMessage)=False
2025-12-01 00:22:47,599 - consoul.ai.history - DEBUG - [TRIM_DEBUG]   Trimmed msg 4: type=tool, isinstance(SystemMessage)=False
2025-12-01 00:22:47,599 - consoul.ai.history - DEBUG - [TRIM_DEBUG] Non-system messages in trimmed result: 5, total original messages: 5
2025-12-01 00:22:47,599 - consoul.tui.app - DEBUG - [MESSAGES] Sending 5 messages to model
2025-12-01 00:22:47,599 - consoul.tui.app - INFO - [IMAGE_DETECTION] Last message has multimodal content: False
2025-12-01 00:22:47,599 - consoul.tui.app - DEBUG - [MESSAGES] Converted 5 messages to 5 dict messages
2025-12-01 00:22:52,045 - consoul.tui.app - DEBUG - [TOOL_FLOW] Stream loop finished. Collected 143 chunks, exception=None
2025-12-01 00:22:52,045 - consoul.tui.app - DEBUG - Found 1 tool_calls after merging chunks
2025-12-01 00:22:52,045 - consoul.tui.app - DEBUG - Final tool_calls: [{'name': 'create_file', 'args': {'content': 'Hello, Claude!', 'dry_run': False, 'file_path': 'hi-claude.md', 'overwrite': True}, 'id': 'a36dca68-08fb-47fa-a983-280fd9052e5e', 'type': 'tool_call'}]
2025-12-01 00:22:52,045 - consoul.tui.app - DEBUG - [TOOL_FLOW] Got first_token: is_none=True, value=None, ttft=4.446s
2025-12-01 00:22:52,046 - consoul.tui.app - DEBUG - [COST] Checking 143 chunks for usage data
2025-12-01 00:22:52,046 - consoul.tui.widgets.chat_view - DEBUG - [SCROLL] Adding message - role: unknown, auto_scroll: True, current_scroll_y: 0.0, max_scroll_y: 0
2025-12-01 00:22:52,046 - consoul.tui.app - DEBUG - [COST] Found usage_metadata in chunk 141: {'input_tokens': 7538, 'output_tokens': 186, 'total_tokens': 7724}
2025-12-01 00:22:52,052 - consoul.tui.app - DEBUG - Final message has 1 tool_calls
2025-12-01 00:22:52,052 - consoul.tui.widgets.chat_view - INFO - [SCROLL] Scheduling scroll_end after message add - role: unknown, scroll_y: 0.0
2025-12-01 00:22:52,052 - consoul.tui.app - DEBUG - [TOOL_FLOW] About to send final_message to main thread
2025-12-01 00:22:52,052 - consoul.tui.widgets.streaming_response - INFO - [SCROLL] Finalizing stream - token_count: 0, height_before: 0, parent_scroll_y: 0.0
2025-12-01 00:22:52,052 - consoul.tui.app - DEBUG - [TOOL_FLOW] Sending final_message to queue: has_message=True
2025-12-01 00:22:52,052 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Stopping auto-scroll timer
2025-12-01 00:22:52,053 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Scrolling self to bottom - height_after: 0, scroll_y: 0.0, max_scroll_y: 0
2025-12-01 00:22:52,053 - consoul.tui.widgets.streaming_response - INFO - [SCROLL] Requesting parent scroll_end - parent_scroll_y: 0.0, parent_max_scroll_y: 0, parent_height: Size(width=170, height=44)
2025-12-01 00:22:52,053 - consoul.tui.app - DEBUG - [TOOL_FLOW] Stream exception: None
2025-12-01 00:22:52,055 - consoul.tui.app - DEBUG - [TOOL_FLOW] Got final_message from queue: type=AIMessage, is_none=False
2025-12-01 00:22:52,055 - consoul.tui.app - DEBUG - [TOOL_FLOW] Response check: has_content=, has_tool_calls_in_message=[{'name': 'create_file', 'args': {'content': 'Hello, Claude!', 'dry_run': False, 'file_path': 'hi-claude.md', 'overwrite': True}, 'id': 'a36dca68-08fb-47fa-a983-280fd9052e5e', 'type': 'tool_call'}], full_response_len=0, cancelled=False
2025-12-01 00:22:52,056 - consoul.tui.app - DEBUG - [TOOL_FLOW] Checking final_message for tool calls: has_final_message=True
2025-12-01 00:22:52,056 - consoul.tui.app - DEBUG - [TOOL_FLOW] Calling has_tool_calls()
2025-12-01 00:22:52,056 - consoul.tui.app - DEBUG - [TOOL_FLOW] Tool calls detected in model response: 1 call(s), content_length=0
2025-12-01 00:22:52,056 - consoul.tui.app - DEBUG - [TOOL_FLOW] Replacing empty stream widget with tool execution message
2025-12-01 00:22:52,057 - consoul.tui.widgets.chat_view - DEBUG - [SCROLL] Adding message - role: system, auto_scroll: True, current_scroll_y: 0.0, max_scroll_y: 0
2025-12-01 00:22:52,060 - consoul.tui.widgets.chat_view - INFO - [SCROLL] Scheduling scroll_end after message add - role: system, scroll_y: 0.0
2025-12-01 00:22:52,060 - consoul.tui.app - DEBUG - [TOOL_FLOW] Calling _handle_tool_calls with 1 calls
2025-12-01 00:22:52,060 - consoul.tui.app - DEBUG - [TOOL_FLOW] Tool call iteration 3 with 1 tool(s)
2025-12-01 00:22:52,060 - consoul.tui.app - DEBUG - [TOOL_FLOW] Reset tool results for new batch of 1 tools
2025-12-01 00:22:52,060 - consoul.tui.app - DEBUG - [TOOL_FLOW] Tool call detected: create_file with args: {'content': 'Hello, Claude!', 'dry_run': False, 'file_path': 'hi-claude.md', 'overwrite': True}
2025-12-01 00:22:52,060 - consoul.tui.app - DEBUG - [TOOL_FLOW] Posting ToolApprovalRequested for create_file
2025-12-01 00:22:52,060 - consoul.tui.app - DEBUG - [TOOL_FLOW] Posted ToolApprovalRequested for create_file
2025-12-01 00:22:52,060 - consoul.tui.app - DEBUG - [TOOL_FLOW] _handle_tool_calls completed
2025-12-01 00:22:52,060 - consoul.tui.app - DEBUG - [SCROLL] Before removing StreamingResponse - scroll_y: 0.0, max_scroll_y: 0, at_bottom: True
2025-12-01 00:22:52,062 - consoul.tui.app - DEBUG - [TOOL_FLOW] on_tool_approval_requested called for create_file
2025-12-01 00:22:52,062 - asyncio - ERROR - _GatheringFuture exception was never retrieved
future: <_GatheringFuture finished exception=TypeError("Input data should be a string, not <class 'rich.text.Text'>")>
Traceback (most recent call last):
  File "/Users/jaredrummler/.pyenv/versions/3.12.3/lib/python3.12/site-packages/textual/widgets/_markdown.py", line 1340, in await_update
    tokens = await asyncio.get_running_loop().run_in_executor(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredrummler/.pyenv/versions/3.12.3/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredrummler/.pyenv/versions/3.12.3/lib/python3.12/site-packages/markdown_it/main.py", line 274, in parse
    raise TypeError(f"Input data should be a string, not {type(src)}")
TypeError: Input data should be a string, not <class 'rich.text.Text'>
2025-12-01 00:22:52,064 - asyncio - ERROR - _GatheringFuture exception was never retrieved
future: <_GatheringFuture finished exception=TypeError("Input data should be a string, not <class 'rich.text.Text'>")>
Traceback (most recent call last):
  File "/Users/jaredrummler/.pyenv/versions/3.12.3/lib/python3.12/site-packages/textual/widgets/_markdown.py", line 1340, in await_update
    tokens = await asyncio.get_running_loop().run_in_executor(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredrummler/.pyenv/versions/3.12.3/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredrummler/.pyenv/versions/3.12.3/lib/python3.12/site-packages/markdown_it/main.py", line 274, in parse
    raise TypeError(f"Input data should be a string, not {type(src)}")
TypeError: Input data should be a string, not <class 'rich.text.Text'>
2025-12-01 00:22:52,064 - consoul.tui.app - DEBUG - [TOOL_FLOW] Checking if approval needed for create_file
2025-12-01 00:22:52,064 - consoul.tui.app - DEBUG - [TOOL_FLOW] Approval check result: needs_approval=False
2025-12-01 00:22:52,064 - consoul.tui.app - DEBUG - [TOOL_FLOW] Auto-approving create_file
2025-12-01 00:22:52,064 - asyncio - ERROR - _GatheringFuture exception was never retrieved
future: <_GatheringFuture finished exception=TypeError("Input data should be a string, not <class 'rich.text.Text'>")>
Traceback (most recent call last):
  File "/Users/jaredrummler/.pyenv/versions/3.12.3/lib/python3.12/site-packages/textual/widgets/_markdown.py", line 1340, in await_update
    tokens = await asyncio.get_running_loop().run_in_executor(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredrummler/.pyenv/versions/3.12.3/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredrummler/.pyenv/versions/3.12.3/lib/python3.12/site-packages/markdown_it/main.py", line 274, in parse
    raise TypeError(f"Input data should be a string, not {type(src)}")
TypeError: Input data should be a string, not <class 'rich.text.Text'>
2025-12-01 00:22:52,068 - consoul.tui.app - DEBUG - [TOOL_FLOW] on_tool_approval_result: tool=create_file, approved=True, call_id=a36dca68-08fb-47fa-a983-280fd9052e5e
2025-12-01 00:22:52,068 - consoul.tui.app - INFO - [TOOL_FLOW] Executing approved tool: create_file
2025-12-01 00:22:52,082 - consoul.tui.app - DEBUG - [TOOL_FLOW] Tool completion status: 1/1 tools completed
2025-12-01 00:22:52,082 - consoul.tui.app - DEBUG - [TOOL_FLOW] All 1 tools completed, posting message to continue
2025-12-01 00:22:52,083 - consoul.tui.app - DEBUG - [TOOL_FLOW] _stream_ai_response ENTRY - iteration 3/5
2025-12-01 00:22:52,083 - consoul.tui.app - DEBUG - [MESSAGES] Before trimming: conversation has 7 messages
2025-12-01 00:22:52,088 - consoul.ai.history - DEBUG - [TRIM_DEBUG] After trimming: 7 messages, original had: 7 messages
2025-12-01 00:22:52,088 - consoul.ai.history - DEBUG - [TRIM_DEBUG] Non-system messages in trimmed result: 7, total original messages: 7
2025-12-01 00:22:52,091 - consoul.tui.app - DEBUG - [MESSAGES] Sending 7 messages to model
2025-12-01 00:22:52,091 - consoul.tui.app - INFO - [IMAGE_DETECTION] Last message has multimodal content: False
2025-12-01 00:22:52,091 - consoul.tui.app - DEBUG - [MESSAGES] Converted 7 messages to 7 dict messages
2025-12-01 00:22:55,939 - consoul.tui.app - DEBUG - [TOOL_FLOW] Got first_token: is_none=False, value=The, ttft=3.848s
2025-12-01 00:22:55,939 - consoul.tui.widgets.chat_view - DEBUG - [SCROLL] Adding message - role: unknown, auto_scroll: True, current_scroll_y: 0.0, max_scroll_y: 0
2025-12-01 00:22:55,942 - consoul.tui.widgets.chat_view - INFO - [SCROLL] Scheduling scroll_end after message add - role: unknown, scroll_y: 0.0
2025-12-01 00:22:55,942 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=3, time_since=1764577375943ms, total_len=3
2025-12-01 00:22:55,942 - consoul.tui.widgets.streaming_response - DEBUG - Rendering full content as markdown: 3 chars
2025-12-01 00:22:55,952 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=5, time_since=10ms, total_len=8
2025-12-01 00:22:55,965 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=7, time_since=23ms, total_len=10
2025-12-01 00:22:55,978 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=9, time_since=36ms, total_len=12
2025-12-01 00:22:55,989 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:55,992 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=10, time_since=50ms, total_len=13
2025-12-01 00:22:56,006 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=13, time_since=63ms, total_len=16
2025-12-01 00:22:56,019 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=16, time_since=77ms, total_len=19
2025-12-01 00:22:56,033 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=19, time_since=91ms, total_len=22
2025-12-01 00:22:56,039 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:56,046 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=20, time_since=104ms, total_len=23
2025-12-01 00:22:56,063 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=28, time_since=121ms, total_len=31
2025-12-01 00:22:56,072 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=35, time_since=130ms, total_len=38
2025-12-01 00:22:56,085 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=38, time_since=143ms, total_len=41
2025-12-01 00:22:56,089 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:56,099 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=42, time_since=157ms, total_len=45
2025-12-01 00:22:56,099 - consoul.tui.widgets.streaming_response - DEBUG - Rendering full content as markdown: 45 chars
2025-12-01 00:22:56,112 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=8, time_since=13ms, total_len=53
2025-12-01 00:22:56,125 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=18, time_since=27ms, total_len=63
2025-12-01 00:22:56,139 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=19, time_since=40ms, total_len=64
2025-12-01 00:22:56,139 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:56,152 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=23, time_since=53ms, total_len=68
2025-12-01 00:22:56,165 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=27, time_since=66ms, total_len=72
2025-12-01 00:22:56,178 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=35, time_since=79ms, total_len=80
2025-12-01 00:22:56,189 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:56,195 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=49, time_since=96ms, total_len=94
2025-12-01 00:22:56,206 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=54, time_since=107ms, total_len=99
2025-12-01 00:22:56,219 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=58, time_since=121ms, total_len=103
2025-12-01 00:22:56,233 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=64, time_since=134ms, total_len=109
2025-12-01 00:22:56,239 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:56,246 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=70, time_since=147ms, total_len=115
2025-12-01 00:22:56,259 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=76, time_since=160ms, total_len=121
2025-12-01 00:22:56,259 - consoul.tui.widgets.streaming_response - DEBUG - Rendering full content as markdown: 121 chars
2025-12-01 00:22:56,273 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=3, time_since=13ms, total_len=124
2025-12-01 00:22:56,286 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=6, time_since=26ms, total_len=127
2025-12-01 00:22:56,289 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:56,299 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=8, time_since=40ms, total_len=129
2025-12-01 00:22:56,312 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=15, time_since=53ms, total_len=136
2025-12-01 00:22:56,325 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=17, time_since=66ms, total_len=138
2025-12-01 00:22:56,339 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=23, time_since=79ms, total_len=144
2025-12-01 00:22:56,339 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:56,352 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=28, time_since=92ms, total_len=149
2025-12-01 00:22:56,365 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=33, time_since=106ms, total_len=154
2025-12-01 00:22:56,378 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=38, time_since=119ms, total_len=159
2025-12-01 00:22:56,389 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:56,391 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=43, time_since=132ms, total_len=164
2025-12-01 00:22:56,406 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=44, time_since=146ms, total_len=165
2025-12-01 00:22:56,418 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=47, time_since=159ms, total_len=168
2025-12-01 00:22:56,418 - consoul.tui.widgets.streaming_response - DEBUG - Rendering full content as markdown: 168 chars
2025-12-01 00:22:56,431 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=6, time_since=13ms, total_len=174
2025-12-01 00:22:56,439 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:56,444 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=11, time_since=26ms, total_len=179
2025-12-01 00:22:56,458 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=14, time_since=39ms, total_len=182
2025-12-01 00:22:56,477 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=21, time_since=59ms, total_len=189
2025-12-01 00:22:56,484 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=25, time_since=66ms, total_len=193
2025-12-01 00:22:56,489 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:56,498 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=34, time_since=80ms, total_len=202
2025-12-01 00:22:56,511 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=39, time_since=93ms, total_len=207
2025-12-01 00:22:56,525 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=45, time_since=106ms, total_len=213
2025-12-01 00:22:56,539 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=47, time_since=121ms, total_len=215
2025-12-01 00:22:56,539 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:56,551 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=49, time_since=133ms, total_len=217
2025-12-01 00:22:56,565 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=56, time_since=147ms, total_len=224
2025-12-01 00:22:56,578 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=59, time_since=160ms, total_len=227
2025-12-01 00:22:56,578 - consoul.tui.widgets.streaming_response - DEBUG - Rendering full content as markdown: 227 chars
2025-12-01 00:22:56,589 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:56,591 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=2, time_since=13ms, total_len=229
2025-12-01 00:22:56,606 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=8, time_since=28ms, total_len=235
2025-12-01 00:22:56,619 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=12, time_since=41ms, total_len=239
2025-12-01 00:22:56,632 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=17, time_since=54ms, total_len=244
2025-12-01 00:22:56,639 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:56,645 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=20, time_since=67ms, total_len=247
2025-12-01 00:22:56,658 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=23, time_since=80ms, total_len=250
2025-12-01 00:22:56,672 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=26, time_since=94ms, total_len=253
2025-12-01 00:22:56,685 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=27, time_since=107ms, total_len=254
2025-12-01 00:22:56,689 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:56,698 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=28, time_since=120ms, total_len=255
2025-12-01 00:22:56,712 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=35, time_since=134ms, total_len=262
2025-12-01 00:22:56,725 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=39, time_since=147ms, total_len=266
2025-12-01 00:22:56,739 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=48, time_since=161ms, total_len=275
2025-12-01 00:22:56,739 - consoul.tui.widgets.streaming_response - DEBUG - Rendering full content as markdown: 275 chars
2025-12-01 00:22:56,740 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:56,752 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=2, time_since=13ms, total_len=277
2025-12-01 00:22:56,766 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=4, time_since=27ms, total_len=279
2025-12-01 00:22:56,778 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=5, time_since=39ms, total_len=280
2025-12-01 00:22:56,789 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:56,792 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=8, time_since=53ms, total_len=283
2025-12-01 00:22:56,806 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=11, time_since=67ms, total_len=286
2025-12-01 00:22:56,819 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=14, time_since=80ms, total_len=289
2025-12-01 00:22:56,832 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=15, time_since=93ms, total_len=290
2025-12-01 00:22:56,839 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:56,845 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=19, time_since=106ms, total_len=294
2025-12-01 00:22:56,858 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=24, time_since=120ms, total_len=299
2025-12-01 00:22:56,872 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=31, time_since=133ms, total_len=306
2025-12-01 00:22:56,885 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=35, time_since=146ms, total_len=310
2025-12-01 00:22:56,889 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:56,899 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=39, time_since=160ms, total_len=314
2025-12-01 00:22:56,899 - consoul.tui.widgets.streaming_response - DEBUG - Rendering full content as markdown: 314 chars
2025-12-01 00:22:56,912 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=5, time_since=13ms, total_len=319
2025-12-01 00:22:56,925 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=10, time_since=26ms, total_len=324
2025-12-01 00:22:56,939 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=14, time_since=40ms, total_len=328
2025-12-01 00:22:56,939 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:56,952 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=22, time_since=53ms, total_len=336
2025-12-01 00:22:56,966 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=24, time_since=67ms, total_len=338
2025-12-01 00:22:56,979 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=29, time_since=80ms, total_len=343
2025-12-01 00:22:56,990 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:56,992 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=30, time_since=93ms, total_len=344
2025-12-01 00:22:57,007 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=37, time_since=108ms, total_len=351
2025-12-01 00:22:57,020 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=38, time_since=121ms, total_len=352
2025-12-01 00:22:57,034 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=40, time_since=135ms, total_len=354
2025-12-01 00:22:57,040 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:57,047 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=43, time_since=148ms, total_len=357
2025-12-01 00:22:57,061 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=45, time_since=162ms, total_len=359
2025-12-01 00:22:57,061 - consoul.tui.widgets.streaming_response - DEBUG - Rendering full content as markdown: 359 chars
2025-12-01 00:22:57,076 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=3, time_since=15ms, total_len=362
2025-12-01 00:22:57,089 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=4, time_since=29ms, total_len=363
2025-12-01 00:22:57,090 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:57,101 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=5, time_since=41ms, total_len=364
2025-12-01 00:22:57,115 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=12, time_since=55ms, total_len=371
2025-12-01 00:22:57,129 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=16, time_since=68ms, total_len=375
2025-12-01 00:22:57,141 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:57,142 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=24, time_since=81ms, total_len=383
2025-12-01 00:22:57,159 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=27, time_since=98ms, total_len=386
2025-12-01 00:22:57,169 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=31, time_since=108ms, total_len=390
2025-12-01 00:22:57,183 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=40, time_since=122ms, total_len=399
2025-12-01 00:22:57,190 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:57,196 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=45, time_since=135ms, total_len=404
2025-12-01 00:22:57,209 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=46, time_since=149ms, total_len=405
2025-12-01 00:22:57,225 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=49, time_since=164ms, total_len=408
2025-12-01 00:22:57,225 - consoul.tui.widgets.streaming_response - DEBUG - Rendering full content as markdown: 408 chars
2025-12-01 00:22:57,236 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=2, time_since=11ms, total_len=410
2025-12-01 00:22:57,240 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:57,249 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=5, time_since=24ms, total_len=413
2025-12-01 00:22:57,262 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=6, time_since=38ms, total_len=414
2025-12-01 00:22:57,275 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=7, time_since=51ms, total_len=415
2025-12-01 00:22:57,290 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:57,290 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=12, time_since=65ms, total_len=420
2025-12-01 00:22:57,302 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=16, time_since=77ms, total_len=424
2025-12-01 00:22:57,315 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=25, time_since=90ms, total_len=433
2025-12-01 00:22:57,329 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=30, time_since=104ms, total_len=438
2025-12-01 00:22:57,340 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:57,342 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=40, time_since=117ms, total_len=448
2025-12-01 00:22:57,356 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=44, time_since=131ms, total_len=452
2025-12-01 00:22:57,369 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=51, time_since=144ms, total_len=459
2025-12-01 00:22:57,382 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=53, time_since=157ms, total_len=461
2025-12-01 00:22:57,382 - consoul.tui.widgets.streaming_response - DEBUG - Rendering full content as markdown: 461 chars
2025-12-01 00:22:57,392 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:57,395 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=10, time_since=13ms, total_len=471
2025-12-01 00:22:57,408 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=15, time_since=26ms, total_len=476
2025-12-01 00:22:57,422 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=17, time_since=40ms, total_len=478
2025-12-01 00:22:57,435 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=18, time_since=53ms, total_len=479
2025-12-01 00:22:57,439 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:57,448 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=20, time_since=66ms, total_len=481
2025-12-01 00:22:57,462 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=22, time_since=80ms, total_len=483
2025-12-01 00:22:57,475 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=24, time_since=93ms, total_len=485
2025-12-01 00:22:57,490 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:57,490 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=26, time_since=108ms, total_len=487
2025-12-01 00:22:57,502 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=27, time_since=120ms, total_len=488
2025-12-01 00:22:57,516 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=30, time_since=134ms, total_len=491
2025-12-01 00:22:57,529 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=33, time_since=147ms, total_len=494
2025-12-01 00:22:57,541 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:57,543 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=37, time_since=161ms, total_len=498
2025-12-01 00:22:57,543 - consoul.tui.widgets.streaming_response - DEBUG - Rendering full content as markdown: 498 chars
2025-12-01 00:22:57,559 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=3, time_since=17ms, total_len=501
2025-12-01 00:22:57,570 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=5, time_since=27ms, total_len=503
2025-12-01 00:22:57,583 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=10, time_since=40ms, total_len=508
2025-12-01 00:22:57,589 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:57,596 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=14, time_since=53ms, total_len=512
2025-12-01 00:22:57,610 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=19, time_since=67ms, total_len=517
2025-12-01 00:22:57,624 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=27, time_since=81ms, total_len=525
2025-12-01 00:22:57,636 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=30, time_since=93ms, total_len=528
2025-12-01 00:22:57,640 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:57,649 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=36, time_since=106ms, total_len=534
2025-12-01 00:22:57,663 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=40, time_since=120ms, total_len=538
2025-12-01 00:22:57,676 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=43, time_since=133ms, total_len=541
2025-12-01 00:22:57,689 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=48, time_since=146ms, total_len=546
2025-12-01 00:22:57,693 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:57,702 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=52, time_since=160ms, total_len=550
2025-12-01 00:22:57,705 - consoul.tui.widgets.streaming_response - DEBUG - Rendering full content as markdown: 550 chars
2025-12-01 00:22:57,716 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=4, time_since=13ms, total_len=554
2025-12-01 00:22:57,729 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=6, time_since=26ms, total_len=556
2025-12-01 00:22:57,740 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:57,742 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=11, time_since=40ms, total_len=561
2025-12-01 00:22:57,757 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=14, time_since=54ms, total_len=564
2025-12-01 00:22:57,769 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=22, time_since=66ms, total_len=572
2025-12-01 00:22:57,784 - consoul.tui.widgets.streaming_response - DEBUG - add_token: buffer_size=23, time_since=81ms, total_len=573
2025-12-01 00:22:57,790 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Auto-scroll during streaming - height: 0, parent_scroll_y: 0.0, parent_max_scroll_y: 0
2025-12-01 00:22:57,797 - consoul.tui.app - DEBUG - [TOOL_FLOW] Stream loop finished. Collected 399 chunks, exception=None
2025-12-01 00:22:57,798 - consoul.tui.app - DEBUG - Found 0 tool_calls after merging chunks
2025-12-01 00:22:57,798 - consoul.tui.app - DEBUG - Final tool_calls: []
2025-12-01 00:22:57,798 - consoul.tui.app - DEBUG - [COST] Checking 399 chunks for usage data
2025-12-01 00:22:57,798 - consoul.tui.widgets.streaming_response - INFO - [SCROLL] Finalizing stream - token_count: 139, height_before: 0, parent_scroll_y: 0.0
2025-12-01 00:22:57,798 - consoul.tui.app - DEBUG - [COST] Found usage_metadata in chunk 397: {'input_tokens': 7626, 'output_tokens': 407, 'total_tokens': 8033}
2025-12-01 00:22:57,799 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Stopping auto-scroll timer
2025-12-01 00:22:57,799 - consoul.tui.app - DEBUG - Final message has 0 tool_calls
2025-12-01 00:22:57,800 - consoul.tui.widgets.streaming_response - DEBUG - [SCROLL] Scrolling self to bottom - height_after: 0, scroll_y: 0.0, max_scroll_y: 0
2025-12-01 00:22:57,800 - consoul.tui.app - DEBUG - [TOOL_FLOW] About to send final_message to main thread
2025-12-01 00:22:57,800 - consoul.tui.widgets.streaming_response - INFO - [SCROLL] Requesting parent scroll_end - parent_scroll_y: 0.0, parent_max_scroll_y: 0, parent_height: Size(width=170, height=44)
2025-12-01 00:22:57,800 - consoul.tui.app - DEBUG - [TOOL_FLOW] Sending final_message to queue: has_message=True
2025-12-01 00:22:57,800 - consoul.tui.app - DEBUG - [TOOL_FLOW] Stream exception: None
2025-12-01 00:22:57,801 - consoul.tui.app - DEBUG - [TOOL_FLOW] Got final_message from queue: type=AIMessage, is_none=False
2025-12-01 00:22:57,801 - consoul.tui.app - DEBUG - [TOOL_FLOW] Response check: has_content=The file `hi-claude.md` already exists in the current directory, and the current configuration does not allow overwriting it.

To create a fresh file with that name, we would need to delete the existing file first (or rename it). Would you like me to:

1. Delete the existing `hi-claude.md` and then create the new file with the content `Hello, Claude!`, **or**
2. Append the content to the existing file, **or**
3. Keep the existing file untouched and create a different file (e.g., `hi-claude-new.md`) with the same content?

Please let me know how youd like to proceed., has_tool_calls_in_message=[], full_response_len=573, cancelled=False
2025-12-01 00:22:57,803 - consoul.tui.app - DEBUG - [TOOL_FLOW] Checking final_message for tool calls: has_final_message=True
2025-12-01 00:22:57,803 - consoul.tui.app - DEBUG - [TOOL_FLOW] Calling has_tool_calls()
2025-12-01 00:22:57,804 - consoul.tui.app - DEBUG - [SCROLL] Before removing StreamingResponse - scroll_y: 0.0, max_scroll_y: 0, at_bottom: True
2025-12-01 00:22:57,811 - consoul.tui.app - DEBUG - [COST] Checking cost calculation: has_final_message=True, has_usage_metadata=True, usage_metadata_value={'input_tokens': 7626, 'output_tokens': 407, 'total_tokens': 8033}
2025-12-01 00:22:57,811 - consoul.tui.app - DEBUG - [COST] Provider check: is_local=True, model_config_type=OllamaModelConfig
2025-12-01 00:22:57,812 - consoul.tui.widgets.chat_view - DEBUG - [SCROLL] Adding message - role: assistant, auto_scroll: False, current_scroll_y: 0.0, max_scroll_y: 0
2025-12-01 00:22:57,819 - consoul.tui.widgets.message_bubble - DEBUG - Metrics text color: #6C757D (theme: consoul-light)
2025-12-01 00:22:57,819 - consoul.tui.widgets.message_bubble - DEBUG - Metrics text color: #6C757D (theme: consoul-light)
2025-12-01 00:22:57,821 - consoul.tui.widgets.message_bubble - DEBUG - Metrics text color: #6C757D (theme: consoul-light)
2025-12-01 00:22:57,821 - consoul.tui.app - DEBUG - [SCROLL] Scheduling scroll after MessageBubble added - current_scroll_y: 0.0, max_scroll_y: 0, expected_min_scroll: 0.0
2025-12-01 00:22:57,831 - consoul.tui.app - DEBUG - [SCROLL] Attempt 1 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:57,846 - consoul.tui.app - DEBUG - [SCROLL] Attempt 2 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:57,858 - consoul.tui.app - DEBUG - [SCROLL] Attempt 3 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:57,875 - consoul.tui.app - DEBUG - [SCROLL] Attempt 4 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:57,909 - consoul.tui.app - DEBUG - [SCROLL] Attempt 5 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:57,920 - consoul.tui.app - DEBUG - [SCROLL] Attempt 6 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:57,935 - consoul.tui.app - DEBUG - [SCROLL] Attempt 7 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:57,945 - consoul.tui.app - DEBUG - [SCROLL] Attempt 8 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:57,954 - consoul.tui.app - DEBUG - [SCROLL] Attempt 9 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:57,965 - consoul.tui.app - DEBUG - [SCROLL] Attempt 10 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:57,975 - consoul.tui.app - DEBUG - [SCROLL] Attempt 11 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:57,986 - consoul.tui.app - DEBUG - [SCROLL] Attempt 12 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:57,996 - consoul.tui.app - DEBUG - [SCROLL] Attempt 13 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,006 - consoul.tui.app - DEBUG - [SCROLL] Attempt 14 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,017 - consoul.tui.app - DEBUG - [SCROLL] Attempt 15 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,027 - consoul.tui.app - DEBUG - [SCROLL] Attempt 16 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,038 - consoul.tui.app - DEBUG - [SCROLL] Attempt 17 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,049 - consoul.tui.app - DEBUG - [SCROLL] Attempt 18 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,058 - consoul.tui.app - DEBUG - [SCROLL] Attempt 19 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,069 - consoul.tui.app - DEBUG - [SCROLL] Attempt 20 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,080 - consoul.tui.app - DEBUG - [SCROLL] Attempt 21 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,089 - consoul.tui.app - DEBUG - [SCROLL] Attempt 22 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,100 - consoul.tui.app - DEBUG - [SCROLL] Attempt 23 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,110 - consoul.tui.app - DEBUG - [SCROLL] Attempt 24 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,121 - consoul.tui.app - DEBUG - [SCROLL] Attempt 25 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,132 - consoul.tui.app - DEBUG - [SCROLL] Attempt 26 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,142 - consoul.tui.app - DEBUG - [SCROLL] Attempt 27 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,152 - consoul.tui.app - DEBUG - [SCROLL] Attempt 28 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,163 - consoul.tui.app - DEBUG - [SCROLL] Attempt 29 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,173 - consoul.tui.app - DEBUG - [SCROLL] Attempt 30 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,183 - consoul.tui.app - DEBUG - [SCROLL] Attempt 31 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,194 - consoul.tui.app - DEBUG - [SCROLL] Attempt 32 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,205 - consoul.tui.app - DEBUG - [SCROLL] Attempt 33 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,216 - consoul.tui.app - DEBUG - [SCROLL] Attempt 34 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,226 - consoul.tui.app - DEBUG - [SCROLL] Attempt 35 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,237 - consoul.tui.app - DEBUG - [SCROLL] Attempt 36 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,248 - consoul.tui.app - DEBUG - [SCROLL] Attempt 37 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,258 - consoul.tui.app - DEBUG - [SCROLL] Attempt 38 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,269 - consoul.tui.app - DEBUG - [SCROLL] Attempt 39 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,280 - consoul.tui.app - DEBUG - [SCROLL] Attempt 40 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,291 - consoul.tui.app - DEBUG - [SCROLL] Attempt 41 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,302 - consoul.tui.app - DEBUG - [SCROLL] Attempt 42 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,313 - consoul.tui.app - DEBUG - [SCROLL] Attempt 43 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,323 - consoul.tui.app - DEBUG - [SCROLL] Attempt 44 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,334 - consoul.tui.app - DEBUG - [SCROLL] Attempt 45 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,344 - consoul.tui.app - DEBUG - [SCROLL] Attempt 46 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,355 - consoul.tui.app - DEBUG - [SCROLL] Attempt 47 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,366 - consoul.tui.app - DEBUG - [SCROLL] Attempt 48 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,377 - consoul.tui.app - DEBUG - [SCROLL] Attempt 49 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,389 - consoul.tui.app - DEBUG - [SCROLL] Attempt 50 - scroll_y: 0.0, max_scroll_y: 0, last_max_scroll: 0, expected_min: 0.0
2025-12-01 00:22:58,389 - consoul.tui.app - DEBUG - [SCROLL] Scrolling to bottom - layout_stable: False, reached_expected: True, timed_out: True, scroll_y: 0.0, max_scroll_y: 0
